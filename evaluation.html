
<!DOCTYPE html>


<html lang="en" data-content_root="./" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>Evaluation module &#8212; Recommenders documentation</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="_static/styles/theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="_static/styles/bootstrap.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="_static/styles/pydata-sphinx-theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />

  
  <link href="_static/vendor/fontawesome/6.5.2/css/all.min.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.5.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.5.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.5.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="_static/pygments.css?v=03e43079" />
    <link rel="stylesheet" type="text/css" href="_static/styles/sphinx-book-theme.css?v=eba8b062" />
    <link rel="stylesheet" type="text/css" href="_static/togglebutton.css?v=13237357" />
    <link rel="stylesheet" type="text/css" href="_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="_static/mystnb.8ecb98da25f57f5357bf6f572d296f466b2cfe2517ffebfabe82451661e28f02.css" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-thebe.css?v=4fa983c6" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-design.min.css?v=95c83b7e" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b" />
<link rel="preload" as="script" href="_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b" />
  <script src="_static/vendor/fontawesome/6.5.2/js/all.min.js?digest=dfe6caa3a7d634c4db9b"></script>

    <script src="_static/documentation_options.js?v=9eb32ce0"></script>
    <script src="_static/doctools.js?v=9a2dae69"></script>
    <script src="_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="_static/copybutton.js?v=f281be69"></script>
    <script src="_static/scripts/sphinx-book-theme.js?v=887ef09a"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="_static/togglebutton.js?v=4a39c7ea"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="_static/design-tabs.js?v=f930bc37"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script async="async" src="_static/sphinx-thebe.js?v=c100c467"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'evaluation';</script>
    <link rel="canonical" href="https://recommenders-team.github.io/recommenders/evaluation.html" />
    <link rel="icon" href="https://raw.githubusercontent.com/recommenders-team/artwork/main/icon/recommenders_color_icon.svg"/>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Recommender algorithms module" href="models.html" />
    <link rel="prev" title="Dataset module" href="datasets.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-primary-sidebar-checkbox"/>
  <label class="overlay overlay-primary" for="pst-primary-sidebar-checkbox"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-secondary-sidebar-checkbox"/>
  <label class="overlay overlay-secondary" for="pst-secondary-sidebar-checkbox"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  
    
  

<a class="navbar-brand logo" href="intro.html">
  
  
  
  
  
    
    
      
    
    
    <img src="https://raw.githubusercontent.com/recommenders-team/artwork/main/color/recommenders_color.svg" class="logo__image only-light" alt="Recommenders documentation - Home"/>
    <script>document.write(`<img src="https://raw.githubusercontent.com/recommenders-team/artwork/main/color/recommenders_color.svg" class="logo__image only-dark" alt="Recommenders documentation - Home"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item">

 <script>
 document.write(`
   <button class="btn search-button-field search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
    <span class="search-button__default-text">Search</span>
    <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
   </button>
 `);
 </script></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">Recommenders API Documentation</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="datasets.html">Dataset module</a></li>
<li class="toctree-l1 current active"><a class="current reference internal" href="#">Evaluation module</a></li>
<li class="toctree-l1"><a class="reference internal" href="models.html">Recommender algorithms module</a></li>
<li class="toctree-l1"><a class="reference internal" href="tuning.html">Hyperparameter tuning module</a></li>
<li class="toctree-l1"><a class="reference internal" href="utils.html">Common utilities module</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><button class="sidebar-toggle primary-toggle btn btn-sm" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</button></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-source-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Source repositories">
    <i class="fab fa-github"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://github.com/recommenders-team/recommenders" target="_blank"
   class="btn btn-sm btn-source-repository-button dropdown-item"
   title="Source repository"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="btn__text-container">Repository</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/recommenders-team/recommenders/issues/new?title=Issue%20on%20page%20%2Fevaluation.html&body=Your%20issue%20content%20here." target="_blank"
   class="btn btn-sm btn-source-issues-button dropdown-item"
   title="Open an issue"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="btn__text-container">Open issue</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="_sources/evaluation.rst" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.rst</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm pst-navbar-icon search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<button class="sidebar-toggle secondary-toggle btn btn-sm" title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</button>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Evaluation module</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#module-recommenders.evaluation.python_evaluation">Python evaluation</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#recommenders.evaluation.python_evaluation.ColumnMismatchError"><code class="docutils literal notranslate"><span class="pre">ColumnMismatchError</span></code></a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#recommenders.evaluation.python_evaluation.ColumnMismatchError.message"><code class="docutils literal notranslate"><span class="pre">ColumnMismatchError.message</span></code></a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#recommenders.evaluation.python_evaluation.ColumnTypeMismatchError"><code class="docutils literal notranslate"><span class="pre">ColumnTypeMismatchError</span></code></a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#recommenders.evaluation.python_evaluation.ColumnTypeMismatchError.message"><code class="docutils literal notranslate"><span class="pre">ColumnTypeMismatchError.message</span></code></a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#recommenders.evaluation.python_evaluation.auc"><code class="docutils literal notranslate"><span class="pre">auc()</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#recommenders.evaluation.python_evaluation.catalog_coverage"><code class="docutils literal notranslate"><span class="pre">catalog_coverage()</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#recommenders.evaluation.python_evaluation.distributional_coverage"><code class="docutils literal notranslate"><span class="pre">distributional_coverage()</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#recommenders.evaluation.python_evaluation.diversity"><code class="docutils literal notranslate"><span class="pre">diversity()</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#recommenders.evaluation.python_evaluation.exp_var"><code class="docutils literal notranslate"><span class="pre">exp_var()</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#recommenders.evaluation.python_evaluation.get_top_k_items"><code class="docutils literal notranslate"><span class="pre">get_top_k_items()</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#recommenders.evaluation.python_evaluation.historical_item_novelty"><code class="docutils literal notranslate"><span class="pre">historical_item_novelty()</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#recommenders.evaluation.python_evaluation.logloss"><code class="docutils literal notranslate"><span class="pre">logloss()</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#recommenders.evaluation.python_evaluation.mae"><code class="docutils literal notranslate"><span class="pre">mae()</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#recommenders.evaluation.python_evaluation.map"><code class="docutils literal notranslate"><span class="pre">map()</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#recommenders.evaluation.python_evaluation.map_at_k"><code class="docutils literal notranslate"><span class="pre">map_at_k()</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#recommenders.evaluation.python_evaluation.merge_ranking_true_pred"><code class="docutils literal notranslate"><span class="pre">merge_ranking_true_pred()</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#recommenders.evaluation.python_evaluation.merge_rating_true_pred"><code class="docutils literal notranslate"><span class="pre">merge_rating_true_pred()</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#recommenders.evaluation.python_evaluation.ndcg_at_k"><code class="docutils literal notranslate"><span class="pre">ndcg_at_k()</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#recommenders.evaluation.python_evaluation.novelty"><code class="docutils literal notranslate"><span class="pre">novelty()</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#recommenders.evaluation.python_evaluation.precision_at_k"><code class="docutils literal notranslate"><span class="pre">precision_at_k()</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#recommenders.evaluation.python_evaluation.r_precision_at_k"><code class="docutils literal notranslate"><span class="pre">r_precision_at_k()</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#recommenders.evaluation.python_evaluation.recall_at_k"><code class="docutils literal notranslate"><span class="pre">recall_at_k()</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#recommenders.evaluation.python_evaluation.rmse"><code class="docutils literal notranslate"><span class="pre">rmse()</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#recommenders.evaluation.python_evaluation.rsquared"><code class="docutils literal notranslate"><span class="pre">rsquared()</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#recommenders.evaluation.python_evaluation.serendipity"><code class="docutils literal notranslate"><span class="pre">serendipity()</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#recommenders.evaluation.python_evaluation.user_diversity"><code class="docutils literal notranslate"><span class="pre">user_diversity()</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#recommenders.evaluation.python_evaluation.user_item_serendipity"><code class="docutils literal notranslate"><span class="pre">user_item_serendipity()</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#recommenders.evaluation.python_evaluation.user_serendipity"><code class="docutils literal notranslate"><span class="pre">user_serendipity()</span></code></a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#module-recommenders.evaluation.spark_evaluation">PySpark evaluation</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#recommenders.evaluation.spark_evaluation.SparkDiversityEvaluation"><code class="docutils literal notranslate"><span class="pre">SparkDiversityEvaluation</span></code></a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#recommenders.evaluation.spark_evaluation.SparkDiversityEvaluation.__init__"><code class="docutils literal notranslate"><span class="pre">SparkDiversityEvaluation.__init__()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#recommenders.evaluation.spark_evaluation.SparkDiversityEvaluation.catalog_coverage"><code class="docutils literal notranslate"><span class="pre">SparkDiversityEvaluation.catalog_coverage()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#recommenders.evaluation.spark_evaluation.SparkDiversityEvaluation.distributional_coverage"><code class="docutils literal notranslate"><span class="pre">SparkDiversityEvaluation.distributional_coverage()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#recommenders.evaluation.spark_evaluation.SparkDiversityEvaluation.diversity"><code class="docutils literal notranslate"><span class="pre">SparkDiversityEvaluation.diversity()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#recommenders.evaluation.spark_evaluation.SparkDiversityEvaluation.historical_item_novelty"><code class="docutils literal notranslate"><span class="pre">SparkDiversityEvaluation.historical_item_novelty()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#recommenders.evaluation.spark_evaluation.SparkDiversityEvaluation.novelty"><code class="docutils literal notranslate"><span class="pre">SparkDiversityEvaluation.novelty()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#recommenders.evaluation.spark_evaluation.SparkDiversityEvaluation.serendipity"><code class="docutils literal notranslate"><span class="pre">SparkDiversityEvaluation.serendipity()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#recommenders.evaluation.spark_evaluation.SparkDiversityEvaluation.user_diversity"><code class="docutils literal notranslate"><span class="pre">SparkDiversityEvaluation.user_diversity()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#recommenders.evaluation.spark_evaluation.SparkDiversityEvaluation.user_item_serendipity"><code class="docutils literal notranslate"><span class="pre">SparkDiversityEvaluation.user_item_serendipity()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#recommenders.evaluation.spark_evaluation.SparkDiversityEvaluation.user_serendipity"><code class="docutils literal notranslate"><span class="pre">SparkDiversityEvaluation.user_serendipity()</span></code></a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#recommenders.evaluation.spark_evaluation.SparkRankingEvaluation"><code class="docutils literal notranslate"><span class="pre">SparkRankingEvaluation</span></code></a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#recommenders.evaluation.spark_evaluation.SparkRankingEvaluation.__init__"><code class="docutils literal notranslate"><span class="pre">SparkRankingEvaluation.__init__()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#recommenders.evaluation.spark_evaluation.SparkRankingEvaluation.map"><code class="docutils literal notranslate"><span class="pre">SparkRankingEvaluation.map()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#recommenders.evaluation.spark_evaluation.SparkRankingEvaluation.map_at_k"><code class="docutils literal notranslate"><span class="pre">SparkRankingEvaluation.map_at_k()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#recommenders.evaluation.spark_evaluation.SparkRankingEvaluation.ndcg_at_k"><code class="docutils literal notranslate"><span class="pre">SparkRankingEvaluation.ndcg_at_k()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#recommenders.evaluation.spark_evaluation.SparkRankingEvaluation.precision_at_k"><code class="docutils literal notranslate"><span class="pre">SparkRankingEvaluation.precision_at_k()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#recommenders.evaluation.spark_evaluation.SparkRankingEvaluation.recall_at_k"><code class="docutils literal notranslate"><span class="pre">SparkRankingEvaluation.recall_at_k()</span></code></a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#recommenders.evaluation.spark_evaluation.SparkRatingEvaluation"><code class="docutils literal notranslate"><span class="pre">SparkRatingEvaluation</span></code></a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#recommenders.evaluation.spark_evaluation.SparkRatingEvaluation.__init__"><code class="docutils literal notranslate"><span class="pre">SparkRatingEvaluation.__init__()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#recommenders.evaluation.spark_evaluation.SparkRatingEvaluation.exp_var"><code class="docutils literal notranslate"><span class="pre">SparkRatingEvaluation.exp_var()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#recommenders.evaluation.spark_evaluation.SparkRatingEvaluation.mae"><code class="docutils literal notranslate"><span class="pre">SparkRatingEvaluation.mae()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#recommenders.evaluation.spark_evaluation.SparkRatingEvaluation.rmse"><code class="docutils literal notranslate"><span class="pre">SparkRatingEvaluation.rmse()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#recommenders.evaluation.spark_evaluation.SparkRatingEvaluation.rsquared"><code class="docutils literal notranslate"><span class="pre">SparkRatingEvaluation.rsquared()</span></code></a></li>
</ul>
</li>
</ul>
</li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section id="evaluation-module">
<span id="evaluation"></span><h1>Evaluation module<a class="headerlink" href="#evaluation-module" title="Link to this heading">#</a></h1>
<section id="module-recommenders.evaluation.python_evaluation">
<span id="python-evaluation"></span><h2>Python evaluation<a class="headerlink" href="#module-recommenders.evaluation.python_evaluation" title="Link to this heading">#</a></h2>
<dl class="py exception">
<dt class="sig sig-object py" id="recommenders.evaluation.python_evaluation.ColumnMismatchError">
<em class="property"><span class="pre">exception</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">recommenders.evaluation.python_evaluation.</span></span><span class="sig-name descname"><span class="pre">ColumnMismatchError</span></span><a class="reference internal" href="_modules/recommenders/evaluation/python_evaluation.html#ColumnMismatchError"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#recommenders.evaluation.python_evaluation.ColumnMismatchError" title="Link to this definition">#</a></dt>
<dd><p>Exception raised when there is a mismatch in columns.</p>
<p>This exception is raised when an operation involving columns
encounters a mismatch or inconsistency.</p>
<dl class="py attribute">
<dt class="sig sig-object py" id="recommenders.evaluation.python_evaluation.ColumnMismatchError.message">
<span class="sig-name descname"><span class="pre">message</span></span><a class="headerlink" href="#recommenders.evaluation.python_evaluation.ColumnMismatchError.message" title="Link to this definition">#</a></dt>
<dd><p>Explanation of the error.</p>
<dl class="field-list simple">
<dt class="field-odd">Type<span class="colon">:</span></dt>
<dd class="field-odd"><p>str</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py exception">
<dt class="sig sig-object py" id="recommenders.evaluation.python_evaluation.ColumnTypeMismatchError">
<em class="property"><span class="pre">exception</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">recommenders.evaluation.python_evaluation.</span></span><span class="sig-name descname"><span class="pre">ColumnTypeMismatchError</span></span><a class="reference internal" href="_modules/recommenders/evaluation/python_evaluation.html#ColumnTypeMismatchError"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#recommenders.evaluation.python_evaluation.ColumnTypeMismatchError" title="Link to this definition">#</a></dt>
<dd><p>Exception raised when there is a mismatch in column types.</p>
<p>This exception is raised when an operation involving column types
encounters a mismatch or inconsistency.</p>
<dl class="py attribute">
<dt class="sig sig-object py" id="recommenders.evaluation.python_evaluation.ColumnTypeMismatchError.message">
<span class="sig-name descname"><span class="pre">message</span></span><a class="headerlink" href="#recommenders.evaluation.python_evaluation.ColumnTypeMismatchError.message" title="Link to this definition">#</a></dt>
<dd><p>Explanation of the error.</p>
<dl class="field-list simple">
<dt class="field-odd">Type<span class="colon">:</span></dt>
<dd class="field-odd"><p>str</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="recommenders.evaluation.python_evaluation.auc">
<span class="sig-prename descclassname"><span class="pre">recommenders.evaluation.python_evaluation.</span></span><span class="sig-name descname"><span class="pre">auc</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">rating_true</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">rating_pred</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">col_user</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'userID'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">col_item</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'itemID'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">col_rating</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'rating'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">col_prediction</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'prediction'</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/recommenders/evaluation/python_evaluation.html#auc"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#recommenders.evaluation.python_evaluation.auc" title="Link to this definition">#</a></dt>
<dd><p>Calculate the Area-Under-Curve metric for implicit feedback typed
recommender, where rating is binary and prediction is float number ranging
from 0 to 1.</p>
<p><a class="reference external" href="https://en.wikipedia.org/wiki/Receiver_operating_characteristic#Area_under_the_curve">https://en.wikipedia.org/wiki/Receiver_operating_characteristic#Area_under_the_curve</a></p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The evaluation does not require a leave-one-out scenario.
This metric does not calculate group-based AUC which considers the AUC scores
averaged across users. It is also not limited to k. Instead, it calculates the
scores on the entire prediction results regardless the users.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>rating_true</strong> (<em>pandas.DataFrame</em>) – True data</p></li>
<li><p><strong>rating_pred</strong> (<em>pandas.DataFrame</em>) – Predicted data</p></li>
<li><p><strong>col_user</strong> (<em>str</em>) – column name for user</p></li>
<li><p><strong>col_item</strong> (<em>str</em>) – column name for item</p></li>
<li><p><strong>col_rating</strong> (<em>str</em>) – column name for rating</p></li>
<li><p><strong>col_prediction</strong> (<em>str</em>) – column name for prediction</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>auc_score (min=0, max=1)</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>float</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="recommenders.evaluation.python_evaluation.catalog_coverage">
<span class="sig-prename descclassname"><span class="pre">recommenders.evaluation.python_evaluation.</span></span><span class="sig-name descname"><span class="pre">catalog_coverage</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">train_df</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">reco_df</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">col_user</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'userID'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">col_item</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'itemID'</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/recommenders/evaluation/python_evaluation.html#catalog_coverage"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#recommenders.evaluation.python_evaluation.catalog_coverage" title="Link to this definition">#</a></dt>
<dd><p>Calculate catalog coverage for recommendations across all users.
The metric definition is based on the “catalog coverage” definition in the following reference:</p>
<dl class="field-list simple">
<dt class="field-odd">Citation<span class="colon">:</span></dt>
<dd class="field-odd"><p>G. Shani and A. Gunawardana, Evaluating Recommendation Systems,
Recommender Systems Handbook pp. 257-297, 2010.</p>
</dd>
<dt class="field-even">Parameters<span class="colon">:</span></dt>
<dd class="field-even"><ul class="simple">
<li><p><strong>train_df</strong> (<em>pandas.DataFrame</em>) – Data set with historical data for users and items they
have interacted with; contains col_user, col_item. Assumed to not contain any duplicate rows.
Interaction here follows the <em>item choice model</em> from Castells et al.</p></li>
<li><p><strong>reco_df</strong> (<em>pandas.DataFrame</em>) – Recommender’s prediction output, containing col_user, col_item,
col_relevance (optional). Assumed to not contain any duplicate user-item pairs.</p></li>
<li><p><strong>col_user</strong> (<em>str</em>) – User id column name.</p></li>
<li><p><strong>col_item</strong> (<em>str</em>) – Item id column name.</p></li>
</ul>
</dd>
<dt class="field-odd">Returns<span class="colon">:</span></dt>
<dd class="field-odd"><p>catalog coverage</p>
</dd>
<dt class="field-even">Return type<span class="colon">:</span></dt>
<dd class="field-even"><p>float</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="recommenders.evaluation.python_evaluation.distributional_coverage">
<span class="sig-prename descclassname"><span class="pre">recommenders.evaluation.python_evaluation.</span></span><span class="sig-name descname"><span class="pre">distributional_coverage</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">train_df</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">reco_df</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">col_user</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'userID'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">col_item</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'itemID'</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/recommenders/evaluation/python_evaluation.html#distributional_coverage"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#recommenders.evaluation.python_evaluation.distributional_coverage" title="Link to this definition">#</a></dt>
<dd><p>Calculate distributional coverage for recommendations across all users.
The metric definition is based on formula (21) in the following reference:</p>
<dl class="field-list simple">
<dt class="field-odd">Citation<span class="colon">:</span></dt>
<dd class="field-odd"><p>G. Shani and A. Gunawardana, Evaluating Recommendation Systems,
Recommender Systems Handbook pp. 257-297, 2010.</p>
</dd>
<dt class="field-even">Parameters<span class="colon">:</span></dt>
<dd class="field-even"><ul class="simple">
<li><p><strong>train_df</strong> (<em>pandas.DataFrame</em>) – Data set with historical data for users and items they
have interacted with; contains col_user, col_item. Assumed to not contain any duplicate rows.
Interaction here follows the <em>item choice model</em> from Castells et al.</p></li>
<li><p><strong>reco_df</strong> (<em>pandas.DataFrame</em>) – Recommender’s prediction output, containing col_user, col_item,
col_relevance (optional). Assumed to not contain any duplicate user-item pairs.</p></li>
<li><p><strong>col_user</strong> (<em>str</em>) – User id column name.</p></li>
<li><p><strong>col_item</strong> (<em>str</em>) – Item id column name.</p></li>
</ul>
</dd>
<dt class="field-odd">Returns<span class="colon">:</span></dt>
<dd class="field-odd"><p>distributional coverage</p>
</dd>
<dt class="field-even">Return type<span class="colon">:</span></dt>
<dd class="field-even"><p>float</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="recommenders.evaluation.python_evaluation.diversity">
<span class="sig-prename descclassname"><span class="pre">recommenders.evaluation.python_evaluation.</span></span><span class="sig-name descname"><span class="pre">diversity</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">train_df</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">reco_df</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">item_feature_df</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">item_sim_measure</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'item_cooccurrence_count'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">col_item_features</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'features'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">col_user</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'userID'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">col_item</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'itemID'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">col_sim</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'sim'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">col_relevance</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/recommenders/evaluation/python_evaluation.html#diversity"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#recommenders.evaluation.python_evaluation.diversity" title="Link to this definition">#</a></dt>
<dd><p>Calculate average diversity of recommendations across all users.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>train_df</strong> (<em>pandas.DataFrame</em>) – Data set with historical data for users and items they have interacted with;
contains col_user, col_item. Assumed to not contain any duplicate rows.</p></li>
<li><p><strong>reco_df</strong> (<em>pandas.DataFrame</em>) – Recommender’s prediction output, containing col_user, col_item, col_relevance (optional).
Assumed to not contain any duplicate user-item pairs.</p></li>
<li><p><strong>item_feature_df</strong> (<em>pandas.DataFrame</em>) – (Optional) It is required only when item_sim_measure=’item_feature_vector’.
It contains two columns: col_item and features (a feature vector).</p></li>
<li><p><strong>item_sim_measure</strong> (<em>str</em>) – (Optional) This column indicates which item similarity measure to be used.
Available measures include item_cooccurrence_count (default choice) and item_feature_vector.</p></li>
<li><p><strong>col_item_features</strong> (<em>str</em>) – item feature column name.</p></li>
<li><p><strong>col_user</strong> (<em>str</em>) – User id column name.</p></li>
<li><p><strong>col_item</strong> (<em>str</em>) – Item id column name.</p></li>
<li><p><strong>col_sim</strong> (<em>str</em>) – This column indicates the column name for item similarity.</p></li>
<li><p><strong>col_relevance</strong> (<em>str</em>) – This column indicates whether the recommended item is actually relevant to the user or not.</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>diversity.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>float</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="recommenders.evaluation.python_evaluation.exp_var">
<span class="sig-prename descclassname"><span class="pre">recommenders.evaluation.python_evaluation.</span></span><span class="sig-name descname"><span class="pre">exp_var</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">rating_true</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">rating_pred</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">col_user</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'userID'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">col_item</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'itemID'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">col_rating</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'rating'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">col_prediction</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'prediction'</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/recommenders/evaluation/python_evaluation.html#exp_var"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#recommenders.evaluation.python_evaluation.exp_var" title="Link to this definition">#</a></dt>
<dd><p>Calculate explained variance.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>rating_true</strong> (<em>pandas.DataFrame</em>) – True data. There should be no duplicate (userID, itemID) pairs</p></li>
<li><p><strong>rating_pred</strong> (<em>pandas.DataFrame</em>) – Predicted data. There should be no duplicate (userID, itemID) pairs</p></li>
<li><p><strong>col_user</strong> (<em>str</em>) – column name for user</p></li>
<li><p><strong>col_item</strong> (<em>str</em>) – column name for item</p></li>
<li><p><strong>col_rating</strong> (<em>str</em>) – column name for rating</p></li>
<li><p><strong>col_prediction</strong> (<em>str</em>) – column name for prediction</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>Explained variance (min=0, max=1).</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>float</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="recommenders.evaluation.python_evaluation.get_top_k_items">
<span class="sig-prename descclassname"><span class="pre">recommenders.evaluation.python_evaluation.</span></span><span class="sig-name descname"><span class="pre">get_top_k_items</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">dataframe</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">col_user</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'userID'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">col_rating</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'rating'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">k</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">10</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/recommenders/evaluation/python_evaluation.html#get_top_k_items"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#recommenders.evaluation.python_evaluation.get_top_k_items" title="Link to this definition">#</a></dt>
<dd><p>Get the input customer-item-rating tuple in the format of Pandas
DataFrame, output a Pandas DataFrame in the dense format of top k items
for each user.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>If it is implicit rating, just append a column of constants to be
ratings.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>dataframe</strong> (<em>pandas.DataFrame</em>) – DataFrame of rating data (in the format</p></li>
<li><p><strong>customerID-itemID-rating</strong><strong>)</strong></p></li>
<li><p><strong>col_user</strong> (<em>str</em>) – column name for user</p></li>
<li><p><strong>col_rating</strong> (<em>str</em>) – column name for rating</p></li>
<li><p><strong>k</strong> (<em>int</em><em> or </em><em>None</em>) – number of items for each user; None means that the input has already been</p></li>
<li><p><strong>again.</strong> (<em>filtered out top k items and sorted by ratings and there is no need to do that</em>)</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>DataFrame of top k items for each user, sorted by <cite>col_user</cite> and <cite>rank</cite></p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>pandas.DataFrame</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="recommenders.evaluation.python_evaluation.historical_item_novelty">
<span class="sig-prename descclassname"><span class="pre">recommenders.evaluation.python_evaluation.</span></span><span class="sig-name descname"><span class="pre">historical_item_novelty</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">train_df</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">reco_df</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">col_user</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'userID'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">col_item</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'itemID'</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/recommenders/evaluation/python_evaluation.html#historical_item_novelty"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#recommenders.evaluation.python_evaluation.historical_item_novelty" title="Link to this definition">#</a></dt>
<dd><p>Calculate novelty for each item. Novelty is computed as the minus logarithm of
(number of interactions with item / total number of interactions). The definition of the metric
is based on the following reference using the choice model (eqs. 1 and 6):</p>
<dl class="field-list simple">
<dt class="field-odd">Citation<span class="colon">:</span></dt>
<dd class="field-odd"><p>P. Castells, S. Vargas, and J. Wang, Novelty and diversity metrics for recommender systems:
choice, discovery and relevance, ECIR 2011</p>
</dd>
</dl>
<p>The novelty of an item can be defined relative to a set of observed events on the set of all items.
These can be events of user choice (item “is picked” by a random user) or user discovery
(item “is known” to a random user). The above definition of novelty reflects a factor of item popularity.
High novelty values correspond to long-tail items in the density function, that few users have interacted
with and low novelty values correspond to popular head items.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>train_df</strong> (<em>pandas.DataFrame</em>) – Data set with historical data for users and items they
have interacted with; contains col_user, col_item. Assumed to not contain any duplicate rows.
Interaction here follows the <em>item choice model</em> from Castells et al.</p></li>
<li><p><strong>reco_df</strong> (<em>pandas.DataFrame</em>) – Recommender’s prediction output, containing col_user, col_item,
col_relevance (optional). Assumed to not contain any duplicate user-item pairs.</p></li>
<li><p><strong>col_user</strong> (<em>str</em>) – User id column name.</p></li>
<li><p><strong>col_item</strong> (<em>str</em>) – Item id column name.</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>A dataframe with the following columns: col_item, item_novelty.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>pandas.DataFrame</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="recommenders.evaluation.python_evaluation.logloss">
<span class="sig-prename descclassname"><span class="pre">recommenders.evaluation.python_evaluation.</span></span><span class="sig-name descname"><span class="pre">logloss</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">rating_true</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">rating_pred</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">col_user</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'userID'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">col_item</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'itemID'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">col_rating</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'rating'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">col_prediction</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'prediction'</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/recommenders/evaluation/python_evaluation.html#logloss"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#recommenders.evaluation.python_evaluation.logloss" title="Link to this definition">#</a></dt>
<dd><p>Calculate the logloss metric for implicit feedback typed
recommender, where rating is binary and prediction is float number ranging
from 0 to 1.</p>
<p><a class="reference external" href="https://en.wikipedia.org/wiki/Loss_functions_for_classification#Cross_entropy_loss_(Log_Loss">https://en.wikipedia.org/wiki/Loss_functions_for_classification#Cross_entropy_loss_(Log_Loss</a>)</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>rating_true</strong> (<em>pandas.DataFrame</em>) – True data</p></li>
<li><p><strong>rating_pred</strong> (<em>pandas.DataFrame</em>) – Predicted data</p></li>
<li><p><strong>col_user</strong> (<em>str</em>) – column name for user</p></li>
<li><p><strong>col_item</strong> (<em>str</em>) – column name for item</p></li>
<li><p><strong>col_rating</strong> (<em>str</em>) – column name for rating</p></li>
<li><p><strong>col_prediction</strong> (<em>str</em>) – column name for prediction</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>log_loss_score (min=-inf, max=inf)</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>float</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="recommenders.evaluation.python_evaluation.mae">
<span class="sig-prename descclassname"><span class="pre">recommenders.evaluation.python_evaluation.</span></span><span class="sig-name descname"><span class="pre">mae</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">rating_true</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">rating_pred</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">col_user</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'userID'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">col_item</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'itemID'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">col_rating</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'rating'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">col_prediction</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'prediction'</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/recommenders/evaluation/python_evaluation.html#mae"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#recommenders.evaluation.python_evaluation.mae" title="Link to this definition">#</a></dt>
<dd><p>Calculate Mean Absolute Error.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>rating_true</strong> (<em>pandas.DataFrame</em>) – True data. There should be no duplicate (userID, itemID) pairs</p></li>
<li><p><strong>rating_pred</strong> (<em>pandas.DataFrame</em>) – Predicted data. There should be no duplicate (userID, itemID) pairs</p></li>
<li><p><strong>col_user</strong> (<em>str</em>) – column name for user</p></li>
<li><p><strong>col_item</strong> (<em>str</em>) – column name for item</p></li>
<li><p><strong>col_rating</strong> (<em>str</em>) – column name for rating</p></li>
<li><p><strong>col_prediction</strong> (<em>str</em>) – column name for prediction</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>Mean Absolute Error.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>float</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="recommenders.evaluation.python_evaluation.map">
<span class="sig-prename descclassname"><span class="pre">recommenders.evaluation.python_evaluation.</span></span><span class="sig-name descname"><span class="pre">map</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">rating_true</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">rating_pred</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">col_user</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'userID'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">col_item</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'itemID'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">col_prediction</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'prediction'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">relevancy_method</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'top_k'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">k</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">10</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">threshold</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">10</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">_</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/recommenders/evaluation/python_evaluation.html#map"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#recommenders.evaluation.python_evaluation.map" title="Link to this definition">#</a></dt>
<dd><p>Mean Average Precision for top k prediction items</p>
<p>The implementation of MAP is referenced from Spark MLlib evaluation metrics.
<a class="reference external" href="https://spark.apache.org/docs/2.3.0/mllib-evaluation-metrics.html#ranking-systems">https://spark.apache.org/docs/2.3.0/mllib-evaluation-metrics.html#ranking-systems</a></p>
<p>A good reference can be found at:
<a class="reference external" href="http://web.stanford.edu/class/cs276/handouts/EvaluationNew-handout-6-per.pdf">http://web.stanford.edu/class/cs276/handouts/EvaluationNew-handout-6-per.pdf</a></p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The MAP is meant to calculate Avg. Precision for the relevant items, so it is normalized by the number of
relevant items in the ground truth data, instead of k.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>rating_true</strong> (<em>pandas.DataFrame</em>) – True DataFrame</p></li>
<li><p><strong>rating_pred</strong> (<em>pandas.DataFrame</em>) – Predicted DataFrame</p></li>
<li><p><strong>col_user</strong> (<em>str</em>) – column name for user</p></li>
<li><p><strong>col_item</strong> (<em>str</em>) – column name for item</p></li>
<li><p><strong>col_prediction</strong> (<em>str</em>) – column name for prediction</p></li>
<li><p><strong>relevancy_method</strong> (<em>str</em>) – method for determining relevancy [‘top_k’, ‘by_threshold’, None]. None means that the
top k items are directly provided, so there is no need to compute the relevancy operation.</p></li>
<li><p><strong>k</strong> (<em>int</em>) – number of top k items per user</p></li>
<li><p><strong>threshold</strong> (<em>float</em>) – threshold of top items per user (optional)</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>MAP (min=0, max=1)</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>float</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="recommenders.evaluation.python_evaluation.map_at_k">
<span class="sig-prename descclassname"><span class="pre">recommenders.evaluation.python_evaluation.</span></span><span class="sig-name descname"><span class="pre">map_at_k</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">rating_true</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">rating_pred</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">col_user</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'userID'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">col_item</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'itemID'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">col_prediction</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'prediction'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">relevancy_method</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'top_k'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">k</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">10</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">threshold</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">10</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">_</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/recommenders/evaluation/python_evaluation.html#map_at_k"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#recommenders.evaluation.python_evaluation.map_at_k" title="Link to this definition">#</a></dt>
<dd><p>Mean Average Precision at k</p>
<p>The implementation of MAP&#64;k is referenced from Spark MLlib evaluation metrics.
<a class="github reference external" href="https://github.com/apache/spark/blob/b938ff9f520fd4e4997938284ffa0aba9ea271fc/mllib/src/main/scala/org/apache/spark/mllib/evaluation/RankingMetrics.scala#L99">apache/spark</a></p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>rating_true</strong> (<em>pandas.DataFrame</em>) – True DataFrame</p></li>
<li><p><strong>rating_pred</strong> (<em>pandas.DataFrame</em>) – Predicted DataFrame</p></li>
<li><p><strong>col_user</strong> (<em>str</em>) – column name for user</p></li>
<li><p><strong>col_item</strong> (<em>str</em>) – column name for item</p></li>
<li><p><strong>col_prediction</strong> (<em>str</em>) – column name for prediction</p></li>
<li><p><strong>relevancy_method</strong> (<em>str</em>) – method for determining relevancy [‘top_k’, ‘by_threshold’, None]. None means that the
top k items are directly provided, so there is no need to compute the relevancy operation.</p></li>
<li><p><strong>k</strong> (<em>int</em>) – number of top k items per user</p></li>
<li><p><strong>threshold</strong> (<em>float</em>) – threshold of top items per user (optional)</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>MAP&#64;k (min=0, max=1)</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>float</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="recommenders.evaluation.python_evaluation.merge_ranking_true_pred">
<span class="sig-prename descclassname"><span class="pre">recommenders.evaluation.python_evaluation.</span></span><span class="sig-name descname"><span class="pre">merge_ranking_true_pred</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">rating_true</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">rating_pred</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">col_user</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">col_item</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">col_prediction</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">relevancy_method</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">k</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">10</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">threshold</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">10</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">_</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/recommenders/evaluation/python_evaluation.html#merge_ranking_true_pred"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#recommenders.evaluation.python_evaluation.merge_ranking_true_pred" title="Link to this definition">#</a></dt>
<dd><p>Filter truth and prediction data frames on common users</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>rating_true</strong> (<em>pandas.DataFrame</em>) – True DataFrame</p></li>
<li><p><strong>rating_pred</strong> (<em>pandas.DataFrame</em>) – Predicted DataFrame</p></li>
<li><p><strong>col_user</strong> (<em>str</em>) – column name for user</p></li>
<li><p><strong>col_item</strong> (<em>str</em>) – column name for item</p></li>
<li><p><strong>col_prediction</strong> (<em>str</em>) – column name for prediction</p></li>
<li><p><strong>relevancy_method</strong> (<em>str</em>) – method for determining relevancy [‘top_k’, ‘by_threshold’, None]. None means that the
top k items are directly provided, so there is no need to compute the relevancy operation.</p></li>
<li><p><strong>k</strong> (<em>int</em>) – number of top k items per user (optional)</p></li>
<li><p><strong>threshold</strong> (<em>float</em>) – threshold of top items per user (optional)</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>DataFrame of recommendation hits, sorted by <cite>col_user</cite> and <cite>rank</cite>
DataFrame of hit counts vs actual relevant items per user number of unique user ids</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>pandas.DataFrame, pandas.DataFrame, int</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="recommenders.evaluation.python_evaluation.merge_rating_true_pred">
<span class="sig-prename descclassname"><span class="pre">recommenders.evaluation.python_evaluation.</span></span><span class="sig-name descname"><span class="pre">merge_rating_true_pred</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">rating_true</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">rating_pred</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">col_user</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'userID'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">col_item</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'itemID'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">col_rating</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'rating'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">col_prediction</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'prediction'</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/recommenders/evaluation/python_evaluation.html#merge_rating_true_pred"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#recommenders.evaluation.python_evaluation.merge_rating_true_pred" title="Link to this definition">#</a></dt>
<dd><p>Join truth and prediction data frames on userID and itemID and return the true
and predicted rated with the correct index.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>rating_true</strong> (<em>pandas.DataFrame</em>) – True data</p></li>
<li><p><strong>rating_pred</strong> (<em>pandas.DataFrame</em>) – Predicted data</p></li>
<li><p><strong>col_user</strong> (<em>str</em>) – column name for user</p></li>
<li><p><strong>col_item</strong> (<em>str</em>) – column name for item</p></li>
<li><p><strong>col_rating</strong> (<em>str</em>) – column name for rating</p></li>
<li><p><strong>col_prediction</strong> (<em>str</em>) – column name for prediction</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>Array with the true ratings
numpy.ndarray: Array with the predicted ratings</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>numpy.ndarray</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="recommenders.evaluation.python_evaluation.ndcg_at_k">
<span class="sig-prename descclassname"><span class="pre">recommenders.evaluation.python_evaluation.</span></span><span class="sig-name descname"><span class="pre">ndcg_at_k</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">rating_true</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">rating_pred</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">col_user</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'userID'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">col_item</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'itemID'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">col_rating</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'rating'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">col_prediction</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'prediction'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">relevancy_method</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'top_k'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">k</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">10</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">threshold</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">10</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">score_type</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'binary'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">discfun_type</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'loge'</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">_</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/recommenders/evaluation/python_evaluation.html#ndcg_at_k"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#recommenders.evaluation.python_evaluation.ndcg_at_k" title="Link to this definition">#</a></dt>
<dd><p>Normalized Discounted Cumulative Gain (nDCG).</p>
<p>Info: <a class="reference external" href="https://en.wikipedia.org/wiki/Discounted_cumulative_gain">https://en.wikipedia.org/wiki/Discounted_cumulative_gain</a></p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>rating_true</strong> (<em>pandas.DataFrame</em>) – True DataFrame</p></li>
<li><p><strong>rating_pred</strong> (<em>pandas.DataFrame</em>) – Predicted DataFrame</p></li>
<li><p><strong>col_user</strong> (<em>str</em>) – column name for user</p></li>
<li><p><strong>col_item</strong> (<em>str</em>) – column name for item</p></li>
<li><p><strong>col_rating</strong> (<em>str</em>) – column name for rating</p></li>
<li><p><strong>col_prediction</strong> (<em>str</em>) – column name for prediction</p></li>
<li><p><strong>relevancy_method</strong> (<em>str</em>) – method for determining relevancy [‘top_k’, ‘by_threshold’, None]. None means that the
top k items are directly provided, so there is no need to compute the relevancy operation.</p></li>
<li><p><strong>k</strong> (<em>int</em>) – number of top k items per user</p></li>
<li><p><strong>threshold</strong> (<em>float</em>) – threshold of top items per user (optional)</p></li>
<li><p><strong>score_type</strong> (<em>str</em>) – type of relevance scores [‘binary’, ‘raw’, ‘exp’]. With the default option ‘binary’, the
relevance score is reduced to either 1 (hit) or 0 (miss). Option ‘raw’ uses the raw relevance score.
Option ‘exp’ uses (2 ** RAW_RELEVANCE - 1) as the relevance score</p></li>
<li><p><strong>discfun_type</strong> (<em>str</em>) – type of discount function [‘loge’, ‘log2’] used to calculate DCG.</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>nDCG at k (min=0, max=1).</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>float</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="recommenders.evaluation.python_evaluation.novelty">
<span class="sig-prename descclassname"><span class="pre">recommenders.evaluation.python_evaluation.</span></span><span class="sig-name descname"><span class="pre">novelty</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">train_df</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">reco_df</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">col_user</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'userID'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">col_item</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'itemID'</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/recommenders/evaluation/python_evaluation.html#novelty"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#recommenders.evaluation.python_evaluation.novelty" title="Link to this definition">#</a></dt>
<dd><p>Calculate the average novelty in a list of recommended items (this assumes that the recommendation list
is already computed). Follows section 5 from</p>
<dl class="field-list simple">
<dt class="field-odd">Citation<span class="colon">:</span></dt>
<dd class="field-odd"><p>P. Castells, S. Vargas, and J. Wang, Novelty and diversity metrics for recommender systems:
choice, discovery and relevance, ECIR 2011</p>
</dd>
<dt class="field-even">Parameters<span class="colon">:</span></dt>
<dd class="field-even"><ul class="simple">
<li><p><strong>train_df</strong> (<em>pandas.DataFrame</em>) – Data set with historical data for users and items they
have interacted with; contains col_user, col_item. Assumed to not contain any duplicate rows.
Interaction here follows the <em>item choice model</em> from Castells et al.</p></li>
<li><p><strong>reco_df</strong> (<em>pandas.DataFrame</em>) – Recommender’s prediction output, containing col_user, col_item,
col_relevance (optional). Assumed to not contain any duplicate user-item pairs.</p></li>
<li><p><strong>col_user</strong> (<em>str</em>) – User id column name.</p></li>
<li><p><strong>col_item</strong> (<em>str</em>) – Item id column name.</p></li>
</ul>
</dd>
<dt class="field-odd">Returns<span class="colon">:</span></dt>
<dd class="field-odd"><p>novelty.</p>
</dd>
<dt class="field-even">Return type<span class="colon">:</span></dt>
<dd class="field-even"><p>float</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="recommenders.evaluation.python_evaluation.precision_at_k">
<span class="sig-prename descclassname"><span class="pre">recommenders.evaluation.python_evaluation.</span></span><span class="sig-name descname"><span class="pre">precision_at_k</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">rating_true</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">rating_pred</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">col_user</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'userID'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">col_item</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'itemID'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">col_prediction</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'prediction'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">relevancy_method</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'top_k'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">k</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">10</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">threshold</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">10</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">_</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/recommenders/evaluation/python_evaluation.html#precision_at_k"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#recommenders.evaluation.python_evaluation.precision_at_k" title="Link to this definition">#</a></dt>
<dd><p>Precision at K.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>We use the same formula to calculate precision&#64;k as that in Spark.
More details can be found at
<a class="reference external" href="http://spark.apache.org/docs/2.1.1/api/python/pyspark.mllib.html#pyspark.mllib.evaluation.RankingMetrics.precisionAt">http://spark.apache.org/docs/2.1.1/api/python/pyspark.mllib.html#pyspark.mllib.evaluation.RankingMetrics.precisionAt</a>
In particular, the maximum achievable precision may be &lt; 1, if the number of items for a
user in rating_pred is less than k.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>rating_true</strong> (<em>pandas.DataFrame</em>) – True DataFrame</p></li>
<li><p><strong>rating_pred</strong> (<em>pandas.DataFrame</em>) – Predicted DataFrame</p></li>
<li><p><strong>col_user</strong> (<em>str</em>) – column name for user</p></li>
<li><p><strong>col_item</strong> (<em>str</em>) – column name for item</p></li>
<li><p><strong>col_prediction</strong> (<em>str</em>) – column name for prediction</p></li>
<li><p><strong>relevancy_method</strong> (<em>str</em>) – method for determining relevancy [‘top_k’, ‘by_threshold’, None]. None means that the
top k items are directly provided, so there is no need to compute the relevancy operation.</p></li>
<li><p><strong>k</strong> (<em>int</em>) – number of top k items per user</p></li>
<li><p><strong>threshold</strong> (<em>float</em>) – threshold of top items per user (optional)</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>precision at k (min=0, max=1)</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>float</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="recommenders.evaluation.python_evaluation.r_precision_at_k">
<span class="sig-prename descclassname"><span class="pre">recommenders.evaluation.python_evaluation.</span></span><span class="sig-name descname"><span class="pre">r_precision_at_k</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">rating_true</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">rating_pred</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">col_user</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'userID'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">col_item</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'itemID'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">col_prediction</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'prediction'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">relevancy_method</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'top_k'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">k</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">10</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">threshold</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">10</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">_</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/recommenders/evaluation/python_evaluation.html#r_precision_at_k"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#recommenders.evaluation.python_evaluation.r_precision_at_k" title="Link to this definition">#</a></dt>
<dd><p>R-precision at K.</p>
<p>R-precision can be defined as the precision&#64;R for each user, where R is the
numer of relevant items for the query. Its also equivalent to the recall at
the R-th position.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>As R can be high, in this case, the k indicates the maximum possible R.
If every user has more than k true items, then r-precision&#64;k is equal to
precision&#64;k. You might need to raise the k value to get meaningful results.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>rating_true</strong> (<em>pandas.DataFrame</em>) – True DataFrame</p></li>
<li><p><strong>rating_pred</strong> (<em>pandas.DataFrame</em>) – Predicted DataFrame</p></li>
<li><p><strong>col_user</strong> (<em>str</em>) – column name for user</p></li>
<li><p><strong>col_item</strong> (<em>str</em>) – column name for item</p></li>
<li><p><strong>col_prediction</strong> (<em>str</em>) – column name for prediction</p></li>
<li><p><strong>relevancy_method</strong> (<em>str</em>) – method for determining relevancy [‘top_k’, ‘by_threshold’, None]. None means that the
top k items are directly provided, so there is no need to compute the relevancy operation.</p></li>
<li><p><strong>k</strong> (<em>int</em>) – number of top k items per user</p></li>
<li><p><strong>threshold</strong> (<em>float</em>) – threshold of top items per user (optional)</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>recall at k (min=0, max=1). The maximum value is 1 even when fewer than
k items exist for a user in rating_true.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>float</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="recommenders.evaluation.python_evaluation.recall_at_k">
<span class="sig-prename descclassname"><span class="pre">recommenders.evaluation.python_evaluation.</span></span><span class="sig-name descname"><span class="pre">recall_at_k</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">rating_true</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">rating_pred</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">col_user</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'userID'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">col_item</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'itemID'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">col_prediction</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'prediction'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">relevancy_method</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'top_k'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">k</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">10</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">threshold</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">10</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">_</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/recommenders/evaluation/python_evaluation.html#recall_at_k"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#recommenders.evaluation.python_evaluation.recall_at_k" title="Link to this definition">#</a></dt>
<dd><p>Recall at K.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>rating_true</strong> (<em>pandas.DataFrame</em>) – True DataFrame</p></li>
<li><p><strong>rating_pred</strong> (<em>pandas.DataFrame</em>) – Predicted DataFrame</p></li>
<li><p><strong>col_user</strong> (<em>str</em>) – column name for user</p></li>
<li><p><strong>col_item</strong> (<em>str</em>) – column name for item</p></li>
<li><p><strong>col_prediction</strong> (<em>str</em>) – column name for prediction</p></li>
<li><p><strong>relevancy_method</strong> (<em>str</em>) – method for determining relevancy [‘top_k’, ‘by_threshold’, None]. None means that the
top k items are directly provided, so there is no need to compute the relevancy operation.</p></li>
<li><p><strong>k</strong> (<em>int</em>) – number of top k items per user</p></li>
<li><p><strong>threshold</strong> (<em>float</em>) – threshold of top items per user (optional)</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>recall at k (min=0, max=1). The maximum value is 1 even when fewer than
k items exist for a user in rating_true.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>float</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="recommenders.evaluation.python_evaluation.rmse">
<span class="sig-prename descclassname"><span class="pre">recommenders.evaluation.python_evaluation.</span></span><span class="sig-name descname"><span class="pre">rmse</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">rating_true</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">rating_pred</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">col_user</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'userID'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">col_item</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'itemID'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">col_rating</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'rating'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">col_prediction</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'prediction'</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/recommenders/evaluation/python_evaluation.html#rmse"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#recommenders.evaluation.python_evaluation.rmse" title="Link to this definition">#</a></dt>
<dd><p>Calculate Root Mean Squared Error</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>rating_true</strong> (<em>pandas.DataFrame</em>) – True data. There should be no duplicate (userID, itemID) pairs</p></li>
<li><p><strong>rating_pred</strong> (<em>pandas.DataFrame</em>) – Predicted data. There should be no duplicate (userID, itemID) pairs</p></li>
<li><p><strong>col_user</strong> (<em>str</em>) – column name for user</p></li>
<li><p><strong>col_item</strong> (<em>str</em>) – column name for item</p></li>
<li><p><strong>col_rating</strong> (<em>str</em>) – column name for rating</p></li>
<li><p><strong>col_prediction</strong> (<em>str</em>) – column name for prediction</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>Root mean squared error</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>float</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="recommenders.evaluation.python_evaluation.rsquared">
<span class="sig-prename descclassname"><span class="pre">recommenders.evaluation.python_evaluation.</span></span><span class="sig-name descname"><span class="pre">rsquared</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">rating_true</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">rating_pred</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">col_user</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'userID'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">col_item</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'itemID'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">col_rating</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'rating'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">col_prediction</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'prediction'</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/recommenders/evaluation/python_evaluation.html#rsquared"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#recommenders.evaluation.python_evaluation.rsquared" title="Link to this definition">#</a></dt>
<dd><p>Calculate R squared</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>rating_true</strong> (<em>pandas.DataFrame</em>) – True data. There should be no duplicate (userID, itemID) pairs</p></li>
<li><p><strong>rating_pred</strong> (<em>pandas.DataFrame</em>) – Predicted data. There should be no duplicate (userID, itemID) pairs</p></li>
<li><p><strong>col_user</strong> (<em>str</em>) – column name for user</p></li>
<li><p><strong>col_item</strong> (<em>str</em>) – column name for item</p></li>
<li><p><strong>col_rating</strong> (<em>str</em>) – column name for rating</p></li>
<li><p><strong>col_prediction</strong> (<em>str</em>) – column name for prediction</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>R squared (min=0, max=1).</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>float</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="recommenders.evaluation.python_evaluation.serendipity">
<span class="sig-prename descclassname"><span class="pre">recommenders.evaluation.python_evaluation.</span></span><span class="sig-name descname"><span class="pre">serendipity</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">train_df</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">reco_df</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">item_feature_df</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">item_sim_measure</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'item_cooccurrence_count'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">col_item_features</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'features'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">col_user</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'userID'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">col_item</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'itemID'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">col_sim</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'sim'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">col_relevance</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/recommenders/evaluation/python_evaluation.html#serendipity"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#recommenders.evaluation.python_evaluation.serendipity" title="Link to this definition">#</a></dt>
<dd><p>Calculate average serendipity for recommendations across all users.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>train_df</strong> (<em>pandas.DataFrame</em>) – Data set with historical data for users and items they
have interacted with; contains col_user, col_item. Assumed to not contain any duplicate rows.</p></li>
<li><p><strong>reco_df</strong> (<em>pandas.DataFrame</em>) – Recommender’s prediction output, containing col_user, col_item,
col_relevance (optional). Assumed to not contain any duplicate user-item pairs.</p></li>
<li><p><strong>item_feature_df</strong> (<em>pandas.DataFrame</em>) – (Optional) It is required only when item_sim_measure=’item_feature_vector’.
It contains two columns: col_item and features (a feature vector).</p></li>
<li><p><strong>item_sim_measure</strong> (<em>str</em>) – (Optional) This column indicates which item similarity measure to be used.
Available measures include item_cooccurrence_count (default choice) and item_feature_vector.</p></li>
<li><p><strong>col_item_features</strong> (<em>str</em>) – item feature column name.</p></li>
<li><p><strong>col_user</strong> (<em>str</em>) – User id column name.</p></li>
<li><p><strong>col_item</strong> (<em>str</em>) – Item id column name.</p></li>
<li><p><strong>col_sim</strong> (<em>str</em>) – This column indicates the column name for item similarity.</p></li>
<li><p><strong>col_relevance</strong> (<em>str</em>) – This column indicates whether the recommended item is actually
relevant to the user or not.</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>serendipity.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>float</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="recommenders.evaluation.python_evaluation.user_diversity">
<span class="sig-prename descclassname"><span class="pre">recommenders.evaluation.python_evaluation.</span></span><span class="sig-name descname"><span class="pre">user_diversity</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">train_df</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">reco_df</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">item_feature_df</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">item_sim_measure</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'item_cooccurrence_count'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">col_item_features</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'features'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">col_user</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'userID'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">col_item</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'itemID'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">col_sim</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'sim'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">col_relevance</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/recommenders/evaluation/python_evaluation.html#user_diversity"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#recommenders.evaluation.python_evaluation.user_diversity" title="Link to this definition">#</a></dt>
<dd><p>Calculate average diversity of recommendations for each user.
The metric definition is based on formula (3) in the following reference:</p>
<dl class="field-list simple">
<dt class="field-odd">Citation<span class="colon">:</span></dt>
<dd class="field-odd"><p>Y.C. Zhang, D.Ó. Séaghdha, D. Quercia and T. Jambor, Auralist:
introducing serendipity into music recommendation, WSDM 2012</p>
</dd>
<dt class="field-even">Parameters<span class="colon">:</span></dt>
<dd class="field-even"><ul class="simple">
<li><p><strong>train_df</strong> (<em>pandas.DataFrame</em>) – Data set with historical data for users and items they have interacted with;
contains col_user, col_item. Assumed to not contain any duplicate rows.</p></li>
<li><p><strong>reco_df</strong> (<em>pandas.DataFrame</em>) – Recommender’s prediction output, containing col_user, col_item, col_relevance (optional).
Assumed to not contain any duplicate user-item pairs.</p></li>
<li><p><strong>item_feature_df</strong> (<em>pandas.DataFrame</em>) – (Optional) It is required only when item_sim_measure=’item_feature_vector’.
It contains two columns: col_item and features (a feature vector).</p></li>
<li><p><strong>item_sim_measure</strong> (<em>str</em>) – (Optional) This column indicates which item similarity measure to be used.
Available measures include item_cooccurrence_count (default choice) and item_feature_vector.</p></li>
<li><p><strong>col_item_features</strong> (<em>str</em>) – item feature column name.</p></li>
<li><p><strong>col_user</strong> (<em>str</em>) – User id column name.</p></li>
<li><p><strong>col_item</strong> (<em>str</em>) – Item id column name.</p></li>
<li><p><strong>col_sim</strong> (<em>str</em>) – This column indicates the column name for item similarity.</p></li>
<li><p><strong>col_relevance</strong> (<em>str</em>) – This column indicates whether the recommended item is actually relevant to the user or not.</p></li>
</ul>
</dd>
<dt class="field-odd">Returns<span class="colon">:</span></dt>
<dd class="field-odd"><p>A dataframe with the following columns: col_user, user_diversity.</p>
</dd>
<dt class="field-even">Return type<span class="colon">:</span></dt>
<dd class="field-even"><p>pandas.DataFrame</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="recommenders.evaluation.python_evaluation.user_item_serendipity">
<span class="sig-prename descclassname"><span class="pre">recommenders.evaluation.python_evaluation.</span></span><span class="sig-name descname"><span class="pre">user_item_serendipity</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">train_df</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">reco_df</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">item_feature_df</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">item_sim_measure</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'item_cooccurrence_count'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">col_item_features</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'features'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">col_user</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'userID'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">col_item</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'itemID'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">col_sim</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'sim'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">col_relevance</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/recommenders/evaluation/python_evaluation.html#user_item_serendipity"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#recommenders.evaluation.python_evaluation.user_item_serendipity" title="Link to this definition">#</a></dt>
<dd><p>Calculate serendipity of each item in the recommendations for each user.
The metric definition is based on the following references:</p>
<dl class="field-list simple">
<dt class="field-odd">Citation<span class="colon">:</span></dt>
<dd class="field-odd"><p></p></dd>
</dl>
<p>Y.C. Zhang, D.Ó. Séaghdha, D. Quercia and T. Jambor, Auralist:
introducing serendipity into music recommendation, WSDM 2012</p>
<p>Eugene Yan, Serendipity: Accuracy’s unpopular best friend in Recommender Systems,
eugeneyan.com, April 2020</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>train_df</strong> (<em>pandas.DataFrame</em>) – Data set with historical data for users and items they
have interacted with; contains col_user, col_item. Assumed to not contain any duplicate rows.</p></li>
<li><p><strong>reco_df</strong> (<em>pandas.DataFrame</em>) – Recommender’s prediction output, containing col_user, col_item,
col_relevance (optional). Assumed to not contain any duplicate user-item pairs.</p></li>
<li><p><strong>item_feature_df</strong> (<em>pandas.DataFrame</em>) – (Optional) It is required only when item_sim_measure=’item_feature_vector’.
It contains two columns: col_item and features (a feature vector).</p></li>
<li><p><strong>item_sim_measure</strong> (<em>str</em>) – (Optional) This column indicates which item similarity measure to be used.
Available measures include item_cooccurrence_count (default choice) and item_feature_vector.</p></li>
<li><p><strong>col_item_features</strong> (<em>str</em>) – item feature column name.</p></li>
<li><p><strong>col_user</strong> (<em>str</em>) – User id column name.</p></li>
<li><p><strong>col_item</strong> (<em>str</em>) – Item id column name.</p></li>
<li><p><strong>col_sim</strong> (<em>str</em>) – This column indicates the column name for item similarity.</p></li>
<li><p><strong>col_relevance</strong> (<em>str</em>) – This column indicates whether the recommended item is actually
relevant to the user or not.</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>A dataframe with columns: col_user, col_item, user_item_serendipity.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>pandas.DataFrame</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="recommenders.evaluation.python_evaluation.user_serendipity">
<span class="sig-prename descclassname"><span class="pre">recommenders.evaluation.python_evaluation.</span></span><span class="sig-name descname"><span class="pre">user_serendipity</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">train_df</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">reco_df</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">item_feature_df</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">item_sim_measure</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'item_cooccurrence_count'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">col_item_features</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'features'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">col_user</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'userID'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">col_item</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'itemID'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">col_sim</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'sim'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">col_relevance</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/recommenders/evaluation/python_evaluation.html#user_serendipity"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#recommenders.evaluation.python_evaluation.user_serendipity" title="Link to this definition">#</a></dt>
<dd><p>Calculate average serendipity for each user’s recommendations.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>train_df</strong> (<em>pandas.DataFrame</em>) – Data set with historical data for users and items they
have interacted with; contains col_user, col_item. Assumed to not contain any duplicate rows.</p></li>
<li><p><strong>reco_df</strong> (<em>pandas.DataFrame</em>) – Recommender’s prediction output, containing col_user, col_item,
col_relevance (optional). Assumed to not contain any duplicate user-item pairs.</p></li>
<li><p><strong>item_feature_df</strong> (<em>pandas.DataFrame</em>) – (Optional) It is required only when item_sim_measure=’item_feature_vector’.
It contains two columns: col_item and features (a feature vector).</p></li>
<li><p><strong>item_sim_measure</strong> (<em>str</em>) – (Optional) This column indicates which item similarity measure to be used.
Available measures include item_cooccurrence_count (default choice) and item_feature_vector.</p></li>
<li><p><strong>col_item_features</strong> (<em>str</em>) – item feature column name.</p></li>
<li><p><strong>col_user</strong> (<em>str</em>) – User id column name.</p></li>
<li><p><strong>col_item</strong> (<em>str</em>) – Item id column name.</p></li>
<li><p><strong>col_sim</strong> (<em>str</em>) – This column indicates the column name for item similarity.</p></li>
<li><p><strong>col_relevance</strong> (<em>str</em>) – This column indicates whether the recommended item is actually
relevant to the user or not.</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>A dataframe with following columns: col_user, user_serendipity.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>pandas.DataFrame</p>
</dd>
</dl>
</dd></dl>

</section>
<section id="module-recommenders.evaluation.spark_evaluation">
<span id="pyspark-evaluation"></span><h2>PySpark evaluation<a class="headerlink" href="#module-recommenders.evaluation.spark_evaluation" title="Link to this heading">#</a></h2>
<dl class="py class">
<dt class="sig sig-object py" id="recommenders.evaluation.spark_evaluation.SparkDiversityEvaluation">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">recommenders.evaluation.spark_evaluation.</span></span><span class="sig-name descname"><span class="pre">SparkDiversityEvaluation</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">train_df</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">reco_df</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">item_feature_df</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">item_sim_measure</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'item_cooccurrence_count'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">col_user</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'userID'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">col_item</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'itemID'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">col_relevance</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/recommenders/evaluation/spark_evaluation.html#SparkDiversityEvaluation"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#recommenders.evaluation.spark_evaluation.SparkDiversityEvaluation" title="Link to this definition">#</a></dt>
<dd><p>Spark Evaluator for diversity, coverage, novelty, serendipity</p>
<dl class="py method">
<dt class="sig sig-object py" id="recommenders.evaluation.spark_evaluation.SparkDiversityEvaluation.__init__">
<span class="sig-name descname"><span class="pre">__init__</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">train_df</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">reco_df</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">item_feature_df</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">item_sim_measure</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'item_cooccurrence_count'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">col_user</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'userID'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">col_item</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'itemID'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">col_relevance</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/recommenders/evaluation/spark_evaluation.html#SparkDiversityEvaluation.__init__"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#recommenders.evaluation.spark_evaluation.SparkDiversityEvaluation.__init__" title="Link to this definition">#</a></dt>
<dd><p>Initializer.</p>
<p>This is the Spark version of diversity metrics evaluator.
The methods of this class calculate the following diversity metrics:</p>
<ul class="simple">
<li><dl class="simple">
<dt>Coverage - it includes two metrics:</dt><dd><ol class="arabic simple">
<li><p>catalog_coverage, which measures the proportion of items that get recommended from the item catalog;</p></li>
<li><p>distributional_coverage, which measures how unequally different items are recommended in the
recommendations to all users.</p></li>
</ol>
</dd>
</dl>
</li>
<li><p>Novelty - A more novel item indicates it is less popular, i.e. it gets recommended less frequently.</p></li>
<li><p>Diversity - The dissimilarity of items being recommended.</p></li>
<li><dl class="simple">
<dt>Serendipity - The “unusualness” or “surprise” of recommendations to a user. When ‘col_relevance’ is used,</dt><dd><p>it indicates how “pleasant surprise” of recommendations is to a user.</p>
</dd>
</dl>
</li>
</ul>
<p>The metric definitions/formulations are based on the following references with modification:</p>
<dl class="field-list">
<dt class="field-odd">Citation<span class="colon">:</span></dt>
<dd class="field-odd"><p>G. Shani and A. Gunawardana, Evaluating Recommendation Systems,
Recommender Systems Handbook pp. 257-297, 2010.</p>
<p>Y.C. Zhang, D.Ó. Séaghdha, D. Quercia and T. Jambor, Auralist: introducing
serendipity into music recommendation, WSDM 2012</p>
<p>P. Castells, S. Vargas, and J. Wang, Novelty and diversity metrics for recommender systems:
choice, discovery and relevance, ECIR 2011</p>
<p>Eugene Yan, Serendipity: Accuracy’s unpopular best friend in Recommender Systems,
eugeneyan.com, April 2020</p>
</dd>
<dt class="field-even">Parameters<span class="colon">:</span></dt>
<dd class="field-even"><ul class="simple">
<li><p><strong>train_df</strong> (<em>pyspark.sql.DataFrame</em>) – Data set with historical data for users and items they
have interacted with; contains col_user, col_item. Assumed to not contain any duplicate rows.
Interaction here follows the <em>item choice model</em> from Castells et al.</p></li>
<li><p><strong>reco_df</strong> (<em>pyspark.sql.DataFrame</em>) – Recommender’s prediction output, containing col_user, col_item,
col_relevance (optional). Assumed to not contain any duplicate user-item pairs.</p></li>
<li><p><strong>item_feature_df</strong> (<em>pyspark.sql.DataFrame</em>) – (Optional) It is required only when item_sim_measure=’item_feature_vector’.
It contains two columns: col_item and features (a feature vector).</p></li>
<li><p><strong>item_sim_measure</strong> (<em>str</em>) – (Optional) This column indicates which item similarity measure to be used.
Available measures include item_cooccurrence_count (default choice) and item_feature_vector.</p></li>
<li><p><strong>col_user</strong> (<em>str</em>) – User id column name.</p></li>
<li><p><strong>col_item</strong> (<em>str</em>) – Item id column name.</p></li>
<li><p><strong>col_relevance</strong> (<em>str</em>) – Optional. This column indicates whether the recommended item is actually
relevant to the user or not.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="recommenders.evaluation.spark_evaluation.SparkDiversityEvaluation.catalog_coverage">
<span class="sig-name descname"><span class="pre">catalog_coverage</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/recommenders/evaluation/spark_evaluation.html#SparkDiversityEvaluation.catalog_coverage"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#recommenders.evaluation.spark_evaluation.SparkDiversityEvaluation.catalog_coverage" title="Link to this definition">#</a></dt>
<dd><p>Calculate catalog coverage for recommendations across all users.
The metric definition is based on the “catalog coverage” definition in the following reference:</p>
<dl class="field-list simple">
<dt class="field-odd">Citation<span class="colon">:</span></dt>
<dd class="field-odd"><p>G. Shani and A. Gunawardana, Evaluating Recommendation Systems,
Recommender Systems Handbook pp. 257-297, 2010.</p>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>catalog coverage</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>float</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="recommenders.evaluation.spark_evaluation.SparkDiversityEvaluation.distributional_coverage">
<span class="sig-name descname"><span class="pre">distributional_coverage</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/recommenders/evaluation/spark_evaluation.html#SparkDiversityEvaluation.distributional_coverage"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#recommenders.evaluation.spark_evaluation.SparkDiversityEvaluation.distributional_coverage" title="Link to this definition">#</a></dt>
<dd><p>Calculate distributional coverage for recommendations across all users.
The metric definition is based on formula (21) in the following reference:</p>
<dl class="field-list simple">
<dt class="field-odd">Citation<span class="colon">:</span></dt>
<dd class="field-odd"><p>G. Shani and A. Gunawardana, Evaluating Recommendation Systems,
Recommender Systems Handbook pp. 257-297, 2010.</p>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>distributional coverage</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>float</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="recommenders.evaluation.spark_evaluation.SparkDiversityEvaluation.diversity">
<span class="sig-name descname"><span class="pre">diversity</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/recommenders/evaluation/spark_evaluation.html#SparkDiversityEvaluation.diversity"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#recommenders.evaluation.spark_evaluation.SparkDiversityEvaluation.diversity" title="Link to this definition">#</a></dt>
<dd><p>Calculate average diversity of recommendations across all users.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns<span class="colon">:</span></dt>
<dd class="field-odd"><p>diversity.</p>
</dd>
<dt class="field-even">Return type<span class="colon">:</span></dt>
<dd class="field-even"><p>float</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="recommenders.evaluation.spark_evaluation.SparkDiversityEvaluation.historical_item_novelty">
<span class="sig-name descname"><span class="pre">historical_item_novelty</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/recommenders/evaluation/spark_evaluation.html#SparkDiversityEvaluation.historical_item_novelty"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#recommenders.evaluation.spark_evaluation.SparkDiversityEvaluation.historical_item_novelty" title="Link to this definition">#</a></dt>
<dd><p>Calculate novelty for each item. Novelty is computed as the minus logarithm of
(number of interactions with item / total number of interactions). The definition of the metric
is based on the following reference using the choice model (eqs. 1 and 6):</p>
<dl class="field-list simple">
<dt class="field-odd">Citation<span class="colon">:</span></dt>
<dd class="field-odd"><p>P. Castells, S. Vargas, and J. Wang, Novelty and diversity metrics for recommender systems:
choice, discovery and relevance, ECIR 2011</p>
</dd>
</dl>
<p>The novelty of an item can be defined relative to a set of observed events on the set of all items.
These can be events of user choice (item “is picked” by a random user) or user discovery
(item “is known” to a random user). The above definition of novelty reflects a factor of item popularity.
High novelty values correspond to long-tail items in the density function, that few users have interacted
with and low novelty values correspond to popular head items.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns<span class="colon">:</span></dt>
<dd class="field-odd"><p>A dataframe with the following columns: col_item, item_novelty.</p>
</dd>
<dt class="field-even">Return type<span class="colon">:</span></dt>
<dd class="field-even"><p>pyspark.sql.dataframe.DataFrame</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="recommenders.evaluation.spark_evaluation.SparkDiversityEvaluation.novelty">
<span class="sig-name descname"><span class="pre">novelty</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/recommenders/evaluation/spark_evaluation.html#SparkDiversityEvaluation.novelty"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#recommenders.evaluation.spark_evaluation.SparkDiversityEvaluation.novelty" title="Link to this definition">#</a></dt>
<dd><p>Calculate the average novelty in a list of recommended items (this assumes that the recommendation list
is already computed). Follows section 5 from</p>
<dl class="field-list simple">
<dt class="field-odd">Citation<span class="colon">:</span></dt>
<dd class="field-odd"><p>P. Castells, S. Vargas, and J. Wang, Novelty and diversity metrics for recommender systems:
choice, discovery and relevance, ECIR 2011</p>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>A dataframe with following columns: novelty.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>pyspark.sql.dataframe.DataFrame</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="recommenders.evaluation.spark_evaluation.SparkDiversityEvaluation.serendipity">
<span class="sig-name descname"><span class="pre">serendipity</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/recommenders/evaluation/spark_evaluation.html#SparkDiversityEvaluation.serendipity"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#recommenders.evaluation.spark_evaluation.SparkDiversityEvaluation.serendipity" title="Link to this definition">#</a></dt>
<dd><p>Calculate average serendipity for recommendations across all users.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns<span class="colon">:</span></dt>
<dd class="field-odd"><p>serendipity.</p>
</dd>
<dt class="field-even">Return type<span class="colon">:</span></dt>
<dd class="field-even"><p>float</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="recommenders.evaluation.spark_evaluation.SparkDiversityEvaluation.user_diversity">
<span class="sig-name descname"><span class="pre">user_diversity</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/recommenders/evaluation/spark_evaluation.html#SparkDiversityEvaluation.user_diversity"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#recommenders.evaluation.spark_evaluation.SparkDiversityEvaluation.user_diversity" title="Link to this definition">#</a></dt>
<dd><p>Calculate average diversity of recommendations for each user.
The metric definition is based on formula (3) in the following reference:</p>
<dl class="field-list simple">
<dt class="field-odd">Citation<span class="colon">:</span></dt>
<dd class="field-odd"><p>Y.C. Zhang, D.Ó. Séaghdha, D. Quercia and T. Jambor, Auralist:
introducing serendipity into music recommendation, WSDM 2012</p>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>A dataframe with the following columns: col_user, user_diversity.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>pyspark.sql.dataframe.DataFrame</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="recommenders.evaluation.spark_evaluation.SparkDiversityEvaluation.user_item_serendipity">
<span class="sig-name descname"><span class="pre">user_item_serendipity</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/recommenders/evaluation/spark_evaluation.html#SparkDiversityEvaluation.user_item_serendipity"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#recommenders.evaluation.spark_evaluation.SparkDiversityEvaluation.user_item_serendipity" title="Link to this definition">#</a></dt>
<dd><p>Calculate serendipity of each item in the recommendations for each user.
The metric definition is based on the following references:</p>
<dl class="field-list">
<dt class="field-odd">Citation<span class="colon">:</span></dt>
<dd class="field-odd"><p>Y.C. Zhang, D.Ó. Séaghdha, D. Quercia and T. Jambor, Auralist:
introducing serendipity into music recommendation, WSDM 2012</p>
<p>Eugene Yan, Serendipity: Accuracy’s unpopular best friend in Recommender Systems,
eugeneyan.com, April 2020</p>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>A dataframe with columns: col_user, col_item, user_item_serendipity.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>pyspark.sql.dataframe.DataFrame</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="recommenders.evaluation.spark_evaluation.SparkDiversityEvaluation.user_serendipity">
<span class="sig-name descname"><span class="pre">user_serendipity</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/recommenders/evaluation/spark_evaluation.html#SparkDiversityEvaluation.user_serendipity"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#recommenders.evaluation.spark_evaluation.SparkDiversityEvaluation.user_serendipity" title="Link to this definition">#</a></dt>
<dd><p>Calculate average serendipity for each user’s recommendations.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns<span class="colon">:</span></dt>
<dd class="field-odd"><p>A dataframe with following columns: col_user, user_serendipity.</p>
</dd>
<dt class="field-even">Return type<span class="colon">:</span></dt>
<dd class="field-even"><p>pyspark.sql.dataframe.DataFrame</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="recommenders.evaluation.spark_evaluation.SparkRankingEvaluation">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">recommenders.evaluation.spark_evaluation.</span></span><span class="sig-name descname"><span class="pre">SparkRankingEvaluation</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">rating_true</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">rating_pred</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">k</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">10</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">relevancy_method</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'top_k'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">col_user</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'userID'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">col_item</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'itemID'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">col_rating</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'rating'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">col_prediction</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'prediction'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">threshold</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">10</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/recommenders/evaluation/spark_evaluation.html#SparkRankingEvaluation"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#recommenders.evaluation.spark_evaluation.SparkRankingEvaluation" title="Link to this definition">#</a></dt>
<dd><p>Spark Ranking Evaluator</p>
<dl class="py method">
<dt class="sig sig-object py" id="recommenders.evaluation.spark_evaluation.SparkRankingEvaluation.__init__">
<span class="sig-name descname"><span class="pre">__init__</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">rating_true</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">rating_pred</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">k</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">10</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">relevancy_method</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'top_k'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">col_user</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'userID'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">col_item</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'itemID'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">col_rating</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'rating'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">col_prediction</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'prediction'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">threshold</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">10</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/recommenders/evaluation/spark_evaluation.html#SparkRankingEvaluation.__init__"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#recommenders.evaluation.spark_evaluation.SparkRankingEvaluation.__init__" title="Link to this definition">#</a></dt>
<dd><p>Initialization.
This is the Spark version of ranking metrics evaluator.
The methods of this class, calculate ranking metrics such as precision&#64;k, recall&#64;k, ndcg&#64;k, and mean average
precision.</p>
<p>The implementations of precision&#64;k, ndcg&#64;k, and mean average precision are referenced from Spark MLlib, which
can be found at <a class="reference external" href="https://spark.apache.org/docs/2.3.0/mllib-evaluation-metrics.html#ranking-systems">the link</a>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>rating_true</strong> (<em>pyspark.sql.DataFrame</em>) – DataFrame of true rating data (in the
format of customerID-itemID-rating tuple).</p></li>
<li><p><strong>rating_pred</strong> (<em>pyspark.sql.DataFrame</em>) – DataFrame of predicted rating data (in
the format of customerID-itemID-rating tuple).</p></li>
<li><p><strong>col_user</strong> (<em>str</em>) – column name for user.</p></li>
<li><p><strong>col_item</strong> (<em>str</em>) – column name for item.</p></li>
<li><p><strong>col_rating</strong> (<em>str</em>) – column name for rating.</p></li>
<li><p><strong>col_prediction</strong> (<em>str</em>) – column name for prediction.</p></li>
<li><p><strong>k</strong> (<em>int</em>) – number of items to recommend to each user.</p></li>
<li><p><strong>relevancy_method</strong> (<em>str</em>) – method for determining relevant items. Possible
values are “top_k”, “by_time_stamp”, and “by_threshold”.</p></li>
<li><p><strong>threshold</strong> (<em>float</em>) – threshold for determining the relevant recommended items.
This is used for the case that predicted ratings follow a known
distribution. NOTE: this option is only activated if <cite>relevancy_method</cite> is
set to “by_threshold”.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="recommenders.evaluation.spark_evaluation.SparkRankingEvaluation.map">
<span class="sig-name descname"><span class="pre">map</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/recommenders/evaluation/spark_evaluation.html#SparkRankingEvaluation.map"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#recommenders.evaluation.spark_evaluation.SparkRankingEvaluation.map" title="Link to this definition">#</a></dt>
<dd><p>Get mean average precision.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns<span class="colon">:</span></dt>
<dd class="field-odd"><p>MAP (min=0, max=1).</p>
</dd>
<dt class="field-even">Return type<span class="colon">:</span></dt>
<dd class="field-even"><p>float</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="recommenders.evaluation.spark_evaluation.SparkRankingEvaluation.map_at_k">
<span class="sig-name descname"><span class="pre">map_at_k</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/recommenders/evaluation/spark_evaluation.html#SparkRankingEvaluation.map_at_k"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#recommenders.evaluation.spark_evaluation.SparkRankingEvaluation.map_at_k" title="Link to this definition">#</a></dt>
<dd><p>Get mean average precision at k.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>More details <a class="reference external" href="http://spark.apache.org/docs/3.0.0/api/python/pyspark.mllib.html#pyspark.mllib.evaluation.RankingMetrics.meanAveragePrecision">on the meanAveragePrecision PySpark documentation</a>.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Returns<span class="colon">:</span></dt>
<dd class="field-odd"><p>MAP at k (min=0, max=1).</p>
</dd>
<dt class="field-even">Return type<span class="colon">:</span></dt>
<dd class="field-even"><p>float</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="recommenders.evaluation.spark_evaluation.SparkRankingEvaluation.ndcg_at_k">
<span class="sig-name descname"><span class="pre">ndcg_at_k</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/recommenders/evaluation/spark_evaluation.html#SparkRankingEvaluation.ndcg_at_k"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#recommenders.evaluation.spark_evaluation.SparkRankingEvaluation.ndcg_at_k" title="Link to this definition">#</a></dt>
<dd><p>Get Normalized Discounted Cumulative Gain (NDCG)</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>More details can be found
<a class="reference external" href="http://spark.apache.org/docs/3.0.0/api/python/pyspark.mllib.html#pyspark.mllib.evaluation.RankingMetrics.ndcgAt">on the ndcgAt PySpark documentation</a>.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Returns<span class="colon">:</span></dt>
<dd class="field-odd"><p>nDCG at k (min=0, max=1).</p>
</dd>
<dt class="field-even">Return type<span class="colon">:</span></dt>
<dd class="field-even"><p>float</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="recommenders.evaluation.spark_evaluation.SparkRankingEvaluation.precision_at_k">
<span class="sig-name descname"><span class="pre">precision_at_k</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/recommenders/evaluation/spark_evaluation.html#SparkRankingEvaluation.precision_at_k"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#recommenders.evaluation.spark_evaluation.SparkRankingEvaluation.precision_at_k" title="Link to this definition">#</a></dt>
<dd><p>Get precision&#64;k.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>More details can be found
<a class="reference external" href="http://spark.apache.org/docs/3.0.0/api/python/pyspark.mllib.html#pyspark.mllib.evaluation.RankingMetrics.precisionAt">on the precisionAt PySpark documentation</a>.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Returns<span class="colon">:</span></dt>
<dd class="field-odd"><p>precision at k (min=0, max=1)</p>
</dd>
<dt class="field-even">Return type<span class="colon">:</span></dt>
<dd class="field-even"><p>float</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="recommenders.evaluation.spark_evaluation.SparkRankingEvaluation.recall_at_k">
<span class="sig-name descname"><span class="pre">recall_at_k</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/recommenders/evaluation/spark_evaluation.html#SparkRankingEvaluation.recall_at_k"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#recommenders.evaluation.spark_evaluation.SparkRankingEvaluation.recall_at_k" title="Link to this definition">#</a></dt>
<dd><p>Get recall&#64;K.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>More details can be found
<a class="reference external" href="http://spark.apache.org/docs/3.0.0/api/python/pyspark.mllib.html#pyspark.mllib.evaluation.RankingMetrics.recallAt">on the recallAt PySpark documentation</a>.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Returns<span class="colon">:</span></dt>
<dd class="field-odd"><p>recall at k (min=0, max=1).</p>
</dd>
<dt class="field-even">Return type<span class="colon">:</span></dt>
<dd class="field-even"><p>float</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="recommenders.evaluation.spark_evaluation.SparkRatingEvaluation">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">recommenders.evaluation.spark_evaluation.</span></span><span class="sig-name descname"><span class="pre">SparkRatingEvaluation</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">rating_true</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">rating_pred</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">col_user</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'userID'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">col_item</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'itemID'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">col_rating</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'rating'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">col_prediction</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'prediction'</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/recommenders/evaluation/spark_evaluation.html#SparkRatingEvaluation"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#recommenders.evaluation.spark_evaluation.SparkRatingEvaluation" title="Link to this definition">#</a></dt>
<dd><p>Spark Rating Evaluator</p>
<dl class="py method">
<dt class="sig sig-object py" id="recommenders.evaluation.spark_evaluation.SparkRatingEvaluation.__init__">
<span class="sig-name descname"><span class="pre">__init__</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">rating_true</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">rating_pred</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">col_user</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'userID'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">col_item</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'itemID'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">col_rating</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'rating'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">col_prediction</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'prediction'</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/recommenders/evaluation/spark_evaluation.html#SparkRatingEvaluation.__init__"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#recommenders.evaluation.spark_evaluation.SparkRatingEvaluation.__init__" title="Link to this definition">#</a></dt>
<dd><p>Initializer.</p>
<p>This is the Spark version of rating metrics evaluator.
The methods of this class, calculate rating metrics such as root mean squared error, mean absolute error,
R squared, and explained variance.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>rating_true</strong> (<em>pyspark.sql.DataFrame</em>) – True labels.</p></li>
<li><p><strong>rating_pred</strong> (<em>pyspark.sql.DataFrame</em>) – Predicted labels.</p></li>
<li><p><strong>col_user</strong> (<em>str</em>) – column name for user.</p></li>
<li><p><strong>col_item</strong> (<em>str</em>) – column name for item.</p></li>
<li><p><strong>col_rating</strong> (<em>str</em>) – column name for rating.</p></li>
<li><p><strong>col_prediction</strong> (<em>str</em>) – column name for prediction.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="recommenders.evaluation.spark_evaluation.SparkRatingEvaluation.exp_var">
<span class="sig-name descname"><span class="pre">exp_var</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/recommenders/evaluation/spark_evaluation.html#SparkRatingEvaluation.exp_var"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#recommenders.evaluation.spark_evaluation.SparkRatingEvaluation.exp_var" title="Link to this definition">#</a></dt>
<dd><p>Calculate explained variance.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Spark MLLib’s implementation is buggy (can lead to values &gt; 1), hence we use var().</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Returns<span class="colon">:</span></dt>
<dd class="field-odd"><p>Explained variance (min=0, max=1).</p>
</dd>
<dt class="field-even">Return type<span class="colon">:</span></dt>
<dd class="field-even"><p>float</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="recommenders.evaluation.spark_evaluation.SparkRatingEvaluation.mae">
<span class="sig-name descname"><span class="pre">mae</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/recommenders/evaluation/spark_evaluation.html#SparkRatingEvaluation.mae"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#recommenders.evaluation.spark_evaluation.SparkRatingEvaluation.mae" title="Link to this definition">#</a></dt>
<dd><p>Calculate Mean Absolute Error.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns<span class="colon">:</span></dt>
<dd class="field-odd"><p>Mean Absolute Error.</p>
</dd>
<dt class="field-even">Return type<span class="colon">:</span></dt>
<dd class="field-even"><p>float</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="recommenders.evaluation.spark_evaluation.SparkRatingEvaluation.rmse">
<span class="sig-name descname"><span class="pre">rmse</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/recommenders/evaluation/spark_evaluation.html#SparkRatingEvaluation.rmse"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#recommenders.evaluation.spark_evaluation.SparkRatingEvaluation.rmse" title="Link to this definition">#</a></dt>
<dd><p>Calculate Root Mean Squared Error.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns<span class="colon">:</span></dt>
<dd class="field-odd"><p>Root mean squared error.</p>
</dd>
<dt class="field-even">Return type<span class="colon">:</span></dt>
<dd class="field-even"><p>float</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="recommenders.evaluation.spark_evaluation.SparkRatingEvaluation.rsquared">
<span class="sig-name descname"><span class="pre">rsquared</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/recommenders/evaluation/spark_evaluation.html#SparkRatingEvaluation.rsquared"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#recommenders.evaluation.spark_evaluation.SparkRatingEvaluation.rsquared" title="Link to this definition">#</a></dt>
<dd><p>Calculate R squared.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns<span class="colon">:</span></dt>
<dd class="field-odd"><p>R squared.</p>
</dd>
<dt class="field-even">Return type<span class="colon">:</span></dt>
<dd class="field-even"><p>float</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./."
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="datasets.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">Dataset module</p>
      </div>
    </a>
    <a class="right-next"
       href="models.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Recommender algorithms module</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#module-recommenders.evaluation.python_evaluation">Python evaluation</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#recommenders.evaluation.python_evaluation.ColumnMismatchError"><code class="docutils literal notranslate"><span class="pre">ColumnMismatchError</span></code></a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#recommenders.evaluation.python_evaluation.ColumnMismatchError.message"><code class="docutils literal notranslate"><span class="pre">ColumnMismatchError.message</span></code></a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#recommenders.evaluation.python_evaluation.ColumnTypeMismatchError"><code class="docutils literal notranslate"><span class="pre">ColumnTypeMismatchError</span></code></a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#recommenders.evaluation.python_evaluation.ColumnTypeMismatchError.message"><code class="docutils literal notranslate"><span class="pre">ColumnTypeMismatchError.message</span></code></a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#recommenders.evaluation.python_evaluation.auc"><code class="docutils literal notranslate"><span class="pre">auc()</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#recommenders.evaluation.python_evaluation.catalog_coverage"><code class="docutils literal notranslate"><span class="pre">catalog_coverage()</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#recommenders.evaluation.python_evaluation.distributional_coverage"><code class="docutils literal notranslate"><span class="pre">distributional_coverage()</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#recommenders.evaluation.python_evaluation.diversity"><code class="docutils literal notranslate"><span class="pre">diversity()</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#recommenders.evaluation.python_evaluation.exp_var"><code class="docutils literal notranslate"><span class="pre">exp_var()</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#recommenders.evaluation.python_evaluation.get_top_k_items"><code class="docutils literal notranslate"><span class="pre">get_top_k_items()</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#recommenders.evaluation.python_evaluation.historical_item_novelty"><code class="docutils literal notranslate"><span class="pre">historical_item_novelty()</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#recommenders.evaluation.python_evaluation.logloss"><code class="docutils literal notranslate"><span class="pre">logloss()</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#recommenders.evaluation.python_evaluation.mae"><code class="docutils literal notranslate"><span class="pre">mae()</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#recommenders.evaluation.python_evaluation.map"><code class="docutils literal notranslate"><span class="pre">map()</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#recommenders.evaluation.python_evaluation.map_at_k"><code class="docutils literal notranslate"><span class="pre">map_at_k()</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#recommenders.evaluation.python_evaluation.merge_ranking_true_pred"><code class="docutils literal notranslate"><span class="pre">merge_ranking_true_pred()</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#recommenders.evaluation.python_evaluation.merge_rating_true_pred"><code class="docutils literal notranslate"><span class="pre">merge_rating_true_pred()</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#recommenders.evaluation.python_evaluation.ndcg_at_k"><code class="docutils literal notranslate"><span class="pre">ndcg_at_k()</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#recommenders.evaluation.python_evaluation.novelty"><code class="docutils literal notranslate"><span class="pre">novelty()</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#recommenders.evaluation.python_evaluation.precision_at_k"><code class="docutils literal notranslate"><span class="pre">precision_at_k()</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#recommenders.evaluation.python_evaluation.r_precision_at_k"><code class="docutils literal notranslate"><span class="pre">r_precision_at_k()</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#recommenders.evaluation.python_evaluation.recall_at_k"><code class="docutils literal notranslate"><span class="pre">recall_at_k()</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#recommenders.evaluation.python_evaluation.rmse"><code class="docutils literal notranslate"><span class="pre">rmse()</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#recommenders.evaluation.python_evaluation.rsquared"><code class="docutils literal notranslate"><span class="pre">rsquared()</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#recommenders.evaluation.python_evaluation.serendipity"><code class="docutils literal notranslate"><span class="pre">serendipity()</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#recommenders.evaluation.python_evaluation.user_diversity"><code class="docutils literal notranslate"><span class="pre">user_diversity()</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#recommenders.evaluation.python_evaluation.user_item_serendipity"><code class="docutils literal notranslate"><span class="pre">user_item_serendipity()</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#recommenders.evaluation.python_evaluation.user_serendipity"><code class="docutils literal notranslate"><span class="pre">user_serendipity()</span></code></a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#module-recommenders.evaluation.spark_evaluation">PySpark evaluation</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#recommenders.evaluation.spark_evaluation.SparkDiversityEvaluation"><code class="docutils literal notranslate"><span class="pre">SparkDiversityEvaluation</span></code></a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#recommenders.evaluation.spark_evaluation.SparkDiversityEvaluation.__init__"><code class="docutils literal notranslate"><span class="pre">SparkDiversityEvaluation.__init__()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#recommenders.evaluation.spark_evaluation.SparkDiversityEvaluation.catalog_coverage"><code class="docutils literal notranslate"><span class="pre">SparkDiversityEvaluation.catalog_coverage()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#recommenders.evaluation.spark_evaluation.SparkDiversityEvaluation.distributional_coverage"><code class="docutils literal notranslate"><span class="pre">SparkDiversityEvaluation.distributional_coverage()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#recommenders.evaluation.spark_evaluation.SparkDiversityEvaluation.diversity"><code class="docutils literal notranslate"><span class="pre">SparkDiversityEvaluation.diversity()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#recommenders.evaluation.spark_evaluation.SparkDiversityEvaluation.historical_item_novelty"><code class="docutils literal notranslate"><span class="pre">SparkDiversityEvaluation.historical_item_novelty()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#recommenders.evaluation.spark_evaluation.SparkDiversityEvaluation.novelty"><code class="docutils literal notranslate"><span class="pre">SparkDiversityEvaluation.novelty()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#recommenders.evaluation.spark_evaluation.SparkDiversityEvaluation.serendipity"><code class="docutils literal notranslate"><span class="pre">SparkDiversityEvaluation.serendipity()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#recommenders.evaluation.spark_evaluation.SparkDiversityEvaluation.user_diversity"><code class="docutils literal notranslate"><span class="pre">SparkDiversityEvaluation.user_diversity()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#recommenders.evaluation.spark_evaluation.SparkDiversityEvaluation.user_item_serendipity"><code class="docutils literal notranslate"><span class="pre">SparkDiversityEvaluation.user_item_serendipity()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#recommenders.evaluation.spark_evaluation.SparkDiversityEvaluation.user_serendipity"><code class="docutils literal notranslate"><span class="pre">SparkDiversityEvaluation.user_serendipity()</span></code></a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#recommenders.evaluation.spark_evaluation.SparkRankingEvaluation"><code class="docutils literal notranslate"><span class="pre">SparkRankingEvaluation</span></code></a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#recommenders.evaluation.spark_evaluation.SparkRankingEvaluation.__init__"><code class="docutils literal notranslate"><span class="pre">SparkRankingEvaluation.__init__()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#recommenders.evaluation.spark_evaluation.SparkRankingEvaluation.map"><code class="docutils literal notranslate"><span class="pre">SparkRankingEvaluation.map()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#recommenders.evaluation.spark_evaluation.SparkRankingEvaluation.map_at_k"><code class="docutils literal notranslate"><span class="pre">SparkRankingEvaluation.map_at_k()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#recommenders.evaluation.spark_evaluation.SparkRankingEvaluation.ndcg_at_k"><code class="docutils literal notranslate"><span class="pre">SparkRankingEvaluation.ndcg_at_k()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#recommenders.evaluation.spark_evaluation.SparkRankingEvaluation.precision_at_k"><code class="docutils literal notranslate"><span class="pre">SparkRankingEvaluation.precision_at_k()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#recommenders.evaluation.spark_evaluation.SparkRankingEvaluation.recall_at_k"><code class="docutils literal notranslate"><span class="pre">SparkRankingEvaluation.recall_at_k()</span></code></a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#recommenders.evaluation.spark_evaluation.SparkRatingEvaluation"><code class="docutils literal notranslate"><span class="pre">SparkRatingEvaluation</span></code></a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#recommenders.evaluation.spark_evaluation.SparkRatingEvaluation.__init__"><code class="docutils literal notranslate"><span class="pre">SparkRatingEvaluation.__init__()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#recommenders.evaluation.spark_evaluation.SparkRatingEvaluation.exp_var"><code class="docutils literal notranslate"><span class="pre">SparkRatingEvaluation.exp_var()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#recommenders.evaluation.spark_evaluation.SparkRatingEvaluation.mae"><code class="docutils literal notranslate"><span class="pre">SparkRatingEvaluation.mae()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#recommenders.evaluation.spark_evaluation.SparkRatingEvaluation.rmse"><code class="docutils literal notranslate"><span class="pre">SparkRatingEvaluation.rmse()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#recommenders.evaluation.spark_evaluation.SparkRatingEvaluation.rsquared"><code class="docutils literal notranslate"><span class="pre">SparkRatingEvaluation.rsquared()</span></code></a></li>
</ul>
</li>
</ul>
</li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Recommenders contributors
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      © Copyright 2018-2024.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b"></script>
<script src="_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>