#data
#data format:sequential model
data: 
    user_vocab : ./tests/resources/deeprec/slirec/user_vocab.pkl # the map file of user to id
    item_vocab : ./tests/resources/deeprec/slirec/item_vocab.pkl # the map file of item to id
    cate_vocab : ./tests/resources/deeprec/slirec/category_vocab.pkl # the map file of category to id

#model
model:
    method : classification # classification or regression
    model_type : SUM
    layer_sizes : [100, 64]  # layers' size of DNN. In this example, DNN has two layers, and each layer has 100 hidden nodes.
    activation : [relu, relu] # activation function for DNN
    user_dropout: True 
    dropout : [0., 0.]  #drop out values for DNN layer
    item_embedding_dim : 32 # the embedding dimension of items
    cate_embedding_dim : 8  # the embedding dimension of categories
    user_embedding_dim : 16 # the embedding dimension of users
    slots: 4 # number of momory channels
    attention_size: 32 # size of hidden layers in attention network
    cell: SUM  ## which type of cell to use in SUM model.  Supported values are ['SUM', 'SUMV2']

#train
#init_method: normal,tnormal,uniform,he_normal,he_uniform,xavier_normal,xavier_uniform
train:
    init_method: tnormal  # method for initializing model parameters
    init_value : 0.01 # stddev values for initializing model parameters
    embed_l2 : 0.0001 # l2 regularization for embedding parameters
    embed_l1 : 0.0000 # l1 regularization for embedding parameters
    layer_l2 : 0.0001 # l2 regularization for hidden layer parameters
    layer_l1 : 0.0000 # l1 regularization for hidden layer parameters
    cross_l2 : 0.0000  # l2 regularization for cross layer parameters
    cross_l1 : 0.000   # l1 regularization for cross layer parameters
    learning_rate : 0.001
    loss : softmax    # pointwise: log_loss, cross_entropy_loss, square_loss  pairwise: softmax
    optimizer : lazyadam  # adam, adadelta, sgd, ftrl, gd, padagrad, pgd, rmsprop, lazyadam
    epochs : 50  #   number of epoch for training
    batch_size : 400  # batch size, should be constrained as an integer multiple of the number of (1 + train_num_ngs) when need_sample is True
    enable_BN : True  # whether to use batch normalization in hidden layers
    EARLY_STOP : 10 # the number of epoch that controls EARLY STOPPING
    max_seq_length : 50 # the maximum number of records in the history sequence
    hidden_size : 40 # the shape of hidden size used in RNN 
    need_sample: True # whether to perform dynamic negative sampling in mini-batch
    train_num_ngs: 4 # indicates how many negative instances followed by one positive instances if need_sample is True

#show info
#metric :'auc', 'logloss', 'group_auc'
info:
    show_step : 100   # print training information after a certain number of mini-batch 
    save_model: True   # whether to save models
    save_epoch : 1    # if save_model is set to True, save the model every save_epoch.
    metrics : ['auc','logloss']  # metrics for evaluation.
    pairwise_metrics : ['mean_mrr', 'ndcg@2;4;6', "group_auc"]  # pairwise metrics for evaluation, available when pairwise comparisons are needed
    MODEL_DIR : ./tests/resources/deeprec/gru4rec/model/gru4rec_model/  # directory of saved models.
    SUMMARIES_DIR : ./tests/resources/deeprec/gru4rec/summary/gru4rec_summary/  # directory of saved summaries.
    write_tfevents : True  # whether to save summaries.

    

