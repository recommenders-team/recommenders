{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<i>Copyright (c) Microsoft Corporation. All rights reserved.<br>\n",
    "Licensed under the MIT License.</i>\n",
    "<br><br>\n",
    "# Recommender Hyperparameter Tuning w/ AzureML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook shows how to auto-tune hyperparameters of a recommender model by utilizing **Azure Machine Learning service**<sup>[a](#azureml-search), [b](#azure-subscription)</sup> ([AzureML](https://azure.microsoft.com/en-us/services/machine-learning-service/)).\n",
    "\n",
    "We present an overall process of utilizing AzureML, specifically [**Hyperdrive**](https://docs.microsoft.com/en-us/python/api/azureml-train-core/azureml.train.hyperdrive?view=azure-ml-py) component, for the hyperparameter tuning by demonstrating key steps:\n",
    "1. Configure AzureML Workspace\n",
    "2. Create Remote Compute Target (GPU cluster)\n",
    "3. Prepare Data\n",
    "4. Prepare Training Scripts\n",
    "5. Setup and Run Hyperdrive Experiment\n",
    "6. Model Import, Re-train and Test\n",
    "\n",
    "In this notebook, we use [**Wide-and-Deep model**](https://ai.googleblog.com/2016/06/wide-deep-learning-better-together-with.html) from **TensorFlow high-level Estimator API (v1.12)** on the movie recommendation scenario. Wide-and-Deep learning jointly trains wide linear model and deep neural networks (DNN) to combine the benefits of memorization and generalization for recommender systems.\n",
    "\n",
    "For more details about the **Wide-and-Deep** model:\n",
    "* [Wide-Deep Quickstart notebook](../00_quick_start/wide_deep_movielens.ipynb)\n",
    "* [Original paper](https://arxiv.org/abs/1606.07792)\n",
    "* [TensorFlow API doc](https://www.tensorflow.org/api_docs/python/tf/estimator/DNNLinearCombinedRegressor)\n",
    "  \n",
    "Regarding **AuzreML**, please refer:\n",
    "* [Quickstart notebook](https://docs.microsoft.com/en-us/azure/machine-learning/service/quickstart-create-workspace-with-python)\n",
    "* [Hyperdrive](https://docs.microsoft.com/en-us/azure/machine-learning/service/how-to-tune-hyperparameters)\n",
    "* [Tensorflow model tuning with Hyperdrive](https://docs.microsoft.com/en-us/azure/machine-learning/service/how-to-train-tensorflow)\n",
    "\n",
    "> <span id=\"azureml-search\">a. </span>To use AzureML, you will need an Azure subscription.  \n",
    "<span id=\"azure-subscription\">b. </span>When you web-search \"Azure Machine Learning\", you will most likely to see mixed results of Azure Machine Learning (AzureML) and Azure Machine Learning **Studio**. Please note they are different services where AzureML's focuses are on ML model management, tracking and hyperparameter tuning, while the [ML Studio](https://studio.azureml.net/)'s is to provide a high-level tool for 'easy-to-use' experience of ML designing and experimentation based on GUI.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# AzureML workspace info. Note, will look up \"aml_config\\config.json\" first, then fall back to use this\n",
    "SUBSCRIPTION_ID = '<subscription-id>'\n",
    "RESOURCE_GROUP  = '<resource-group>'\n",
    "WORKSPACE_NAME  = '<workspace-name>'\n",
    "\n",
    "# Remote compute (cluster) configuration. If you want to save the cost more, set these to small.\n",
    "VM_SIZE = 'STANDARD_NC6'\n",
    "VM_PRIORITY = 'lowpriority'\n",
    "# Cluster nodes\n",
    "MIN_NODES = 4\n",
    "MAX_NODES = 8\n",
    "# Hyperdrive experimentation configuration\n",
    "MAX_TOTAL_RUNS = 100  # Number of runs (training-and-evaluation) to search the best hyperparameters. \n",
    "MAX_CONCURRENT_RUNS = 4\n",
    "\n",
    "# Recommend top k items\n",
    "TOP_K = 10\n",
    "# Select Movielens data size: 100k, 1m, 10m, or 20m\n",
    "MOVIELENS_DATA_SIZE = '100k'\n",
    "EPOCHS = 50\n",
    "# Metrics to track. Put your primary metric (metric to optimize) at the first place.\n",
    "METRICS = ['rmse', 'mae', 'ndcg', 'precision']\n",
    "# Data column names\n",
    "USER_COL = 'UserId'\n",
    "ITEM_COL = 'MovieId'\n",
    "RATING_COL = 'Rating'\n",
    "ITEM_FEAT_COL = 'Genres'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Configure AzureML Workspace\n",
    "**AzureML workspace** is a foundational block in the cloud that you use to experiment, train, and deploy machine learning models via AzureML service. In this notebook, we 1) create a workspace from [**Azure portal**](https://portal.azure.com) and 2) configure from this notebook.\n",
    "\n",
    "You can find more details about the setup and configure processes from the following links:\n",
    "* [Quickstart with Azure portal](https://docs.microsoft.com/en-us/azure/machine-learning/service/quickstart-get-started)\n",
    "* [Quickstart with Python SDK](https://docs.microsoft.com/en-us/azure/machine-learning/service/quickstart-create-workspace-with-python)\n",
    "  \n",
    "<br>\n",
    "  \n",
    "#### 1.1 Create a workspace\n",
    "1. Sign in to the [Azure portal](https://portal.azure.com) by using the credentials for the Azure subscription you use.\n",
    "2. Select **Create a resource** menu, search for **Machine Learning service workspace** select **Create** button.\n",
    "3. In the **ML service workspace** pane, configure your workspace with entering the *workspace name* and *resource group* (or **create new** resource group if you don't have one already), and select **Create**. It can take a few moments to create the workspace.\n",
    "  \n",
    "<br>\n",
    "  \n",
    "#### 1.2 Configure\n",
    "To configure this notebook to communicate with the workspace, type in your Azure subscription id, the resource group name and workspace name to `<subscription-id>`, `<resource-group>`, `<workspace-name>` in the above notebook cell. Alternatively, you can create a *.\\aml_config\\config.json* file with the following contents:\n",
    "```\n",
    "{\n",
    "    \"subscription_id\": \"<subscription-id>\",\n",
    "    \"resource_group\": \"<resource-group>\",\n",
    "    \"workspace_name\": \"<workspace-name>\"\n",
    "}\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's see if everything is ready!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Azure ML SDK Version: 1.0.10\n",
      "Tensorflow Version: 1.12.0\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"../../\")\n",
    "\n",
    "import itertools\n",
    "import os\n",
    "import shutil\n",
    "import time\n",
    "\n",
    "from IPython.display import clear_output\n",
    "import numpy as np\n",
    "import papermill as pm\n",
    "import pandas as pd\n",
    "import sklearn.preprocessing\n",
    "import tensorflow as tf\n",
    "\n",
    "import azureml as aml\n",
    "import azureml.widgets as widgets\n",
    "import azureml.train.hyperdrive as hd\n",
    "\n",
    "from reco_utils.dataset.pandas_df_utils import user_item_pairs\n",
    "from reco_utils.dataset import movielens\n",
    "from reco_utils.dataset.python_splitters import python_random_split\n",
    "from reco_utils.common import tf_utils\n",
    "from reco_utils.evaluation.python_evaluation import (\n",
    "    rmse, mae, rsquared, exp_var,\n",
    "    map_at_k, ndcg_at_k, precision_at_k, recall_at_k\n",
    ")\n",
    "\n",
    "RANKING_METRICS = {\n",
    "    'map': map_at_k,\n",
    "    'ndcg': ndcg_at_k,\n",
    "    'precision': precision_at_k,\n",
    "    'recall': recall_at_k\n",
    "}\n",
    "RATING_METRICS = {\n",
    "    'rmse': rmse,\n",
    "    'mae': mae,\n",
    "    'rsquared': rsquared,\n",
    "    'exp_var': exp_var\n",
    "}\n",
    "\n",
    "print(\"Azure ML SDK Version:\", aml.core.VERSION)\n",
    "print(\"Tensorflow Version:\", tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to a workspace\n",
    "try:\n",
    "    ws = aml.core.Workspace.from_config()\n",
    "except aml.exceptions.UserErrorException:\n",
    "    try:\n",
    "        ws = aml.core.Workspace(\n",
    "            subscription_id=SUBSCRIPTION_ID,\n",
    "            resource_group=RESOURCE_GROUP,\n",
    "            workspace_name=WORKSPACE_NAME\n",
    "        )\n",
    "        ws.write_config()\n",
    "    except aml.exceptions.AuthenticationException:\n",
    "        ws = None\n",
    "\n",
    "if ws is None:\n",
    "    raise ValueError(\n",
    "        \"\"\"Cannot access the AzureML workspace w/ the config info provided.\n",
    "        Please check if you entered the correct id, group name and workspace name\"\"\"\n",
    "    )\n",
    "else:\n",
    "    print(\"AzureML workspace name: \", ws.name)\n",
    "    clear_output()  # Comment out this if you want to see your workspace info."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Create Remote Compute Target\n",
    "\n",
    "We create a gpu cluster as our **remote compute target**. If a cluster with the same name is already exist in your workspace, the script will load it instead. You can see [this document](https://docs.microsoft.com/en-us/azure/machine-learning/service/how-to-set-up-training-targets) to learn more about setting up a compute target on different locations.\n",
    "\n",
    "This notebook selects **STANDARD_NC6** virtual machine (VM) and sets it's priority as *lowpriority* to save the cost.\n",
    "\n",
    "Size | vCPU | Memory (GiB) | Temp storage (SSD, GiB) | GPU | GPU memory (GiB) | Max data disks | Max NICs\n",
    "---|---|---|---|---|---|---|---\n",
    "Standard_NC6 | <center>6</center> | <center>56</center> | <center>340</center> | <center>1</center> | <center>8</center> | <center>24</center> | <center>1</center>\n",
    "\n",
    "For more information about Azure virtual machine sizes, see [here](https://docs.microsoft.com/en-us/azure/virtual-machines/windows/sizes-gpu)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing compute target\n",
      "{'allocationState': 'Steady', 'allocationStateTransitionTime': '2019-02-11T09:51:57.871000+00:00', 'creationTime': '2019-02-04T16:59:49.711395+00:00', 'currentNodeCount': 4, 'errors': None, 'modifiedTime': '2019-02-04T17:00:42.840716+00:00', 'nodeStateCounts': {'idleNodeCount': 4, 'leavingNodeCount': 0, 'preemptedNodeCount': 0, 'preparingNodeCount': 0, 'runningNodeCount': 0, 'unusableNodeCount': 0}, 'provisioningState': 'Succeeded', 'provisioningStateTransitionTime': None, 'scaleSettings': {'minNodeCount': 4, 'maxNodeCount': 8, 'nodeIdleTimeBeforeScaleDown': 'PT120S'}, 'targetNodeCount': 4, 'vmPriority': 'LowPriority', 'vmSize': 'STANDARD_NC6'}\n"
     ]
    }
   ],
   "source": [
    "CLUSTER_NAME = 'gpu-cluster-nc6'\n",
    "\n",
    "try:\n",
    "    compute_target = aml.core.compute.ComputeTarget(workspace=ws, name=CLUSTER_NAME)\n",
    "    print(\"Found existing compute target\")\n",
    "except aml.core.compute_target.ComputeTargetException:\n",
    "    print(\"Creating a new compute target...\")\n",
    "    compute_config = aml.core.compute.AmlCompute.provisioning_configuration(\n",
    "        vm_size=VM_SIZE,\n",
    "        vm_priority=VM_PRIORITY,\n",
    "        min_nodes=MIN_NODES,\n",
    "        max_nodes=MAX_NODES\n",
    "    )\n",
    "    # create the cluster\n",
    "    compute_target = aml.core.compute.ComputeTarget.create(ws, CLUSTER_NAME, compute_config)\n",
    "    compute_target.wait_for_completion(show_output=True, min_node_count=None, timeout_in_minutes=20)\n",
    "\n",
    "# Use the 'status' property to get a detailed status for the current cluster. \n",
    "print(compute_target.status.serialize())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Prepare Data\n",
    "For demonstration purpose, we use 100k MovieLens dataset. First, download the data and convert the format (multi-hot encode *genres*) to make it work for our model. More details about this step is described in our [Wide-Deep Quickstart notebook](../00_quick_start/wide_deep_movielens.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>UserId</th>\n",
       "      <th>MovieId</th>\n",
       "      <th>Rating</th>\n",
       "      <th>Genres</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>196</td>\n",
       "      <td>242</td>\n",
       "      <td>3.0</td>\n",
       "      <td>[0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>63</td>\n",
       "      <td>242</td>\n",
       "      <td>3.0</td>\n",
       "      <td>[0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>226</td>\n",
       "      <td>242</td>\n",
       "      <td>5.0</td>\n",
       "      <td>[0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>154</td>\n",
       "      <td>242</td>\n",
       "      <td>3.0</td>\n",
       "      <td>[0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>306</td>\n",
       "      <td>242</td>\n",
       "      <td>5.0</td>\n",
       "      <td>[0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   UserId  MovieId  Rating                                             Genres\n",
       "0     196      242     3.0  [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
       "1      63      242     3.0  [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
       "2     226      242     5.0  [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
       "3     154      242     3.0  [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
       "4     306      242     5.0  [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ..."
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = movielens.load_pandas_df(\n",
    "    size=MOVIELENS_DATA_SIZE,\n",
    "    header=[USER_COL, ITEM_COL, RATING_COL],\n",
    "    genres_col='Genres_string'\n",
    ")\n",
    "\n",
    "# Encode 'genres' into int array (multi-hot representation) to use as item features\n",
    "genres_encoder = sklearn.preprocessing.MultiLabelBinarizer()\n",
    "data[ITEM_FEAT_COL] = genres_encoder.fit_transform(\n",
    "    data['Genres_string'].apply(lambda s: s.split(\"|\"))\n",
    ").tolist()\n",
    "data.drop('Genres_string', axis=1, inplace=True)\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset is split into train, validation, and test sets. The train and validation sets will be used for hyperparameter tuning, and the test set will be used for the final evaluation of the model after we import the best model from AzureML workspace.\n",
    "\n",
    "Here, we don't use multiple-split directly by passing `ratio=[0.56, 0.19, 0.25]`. Instead, we first split the data into train and test sets with the same `seed` we've been using in other notebooks to make the train set identical across them. Then, we further split the train set into train and validation sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "56250 18750 25000\n"
     ]
    }
   ],
   "source": [
    "# Use the same seed to make the train and test sets identical across other notebooks in the repo.\n",
    "train, test = python_random_split(data, ratio=0.75, seed=123)\n",
    "# Further split the train set into train and validation set.\n",
    "train, valid = python_random_split(train)\n",
    "\n",
    "print(len(train), len(valid), len(test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, upload the train and validation sets to the AzureML workspace. Our Hyperdrivce experiment will use them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploading aml_data/movielens_100k_train.pkl\n",
      "Uploading aml_data/movielens_100k_valid.pkl\n",
      "Uploaded aml_data/movielens_100k_valid.pkl, 1 files out of an estimated total of 2\n",
      "Uploaded aml_data/movielens_100k_train.pkl, 2 files out of an estimated total of 2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "$AZUREML_DATAREFERENCE_2a7a3cb4cf38480abd3eda88df08b2e1"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DATA_DIR = 'aml_data'\n",
    "\n",
    "os.makedirs(DATA_DIR, exist_ok=True)\n",
    "\n",
    "TRAIN_FILE_NAME = \"movielens_\" + MOVIELENS_DATA_SIZE + \"_train.pkl\"\n",
    "train.to_pickle(os.path.join(DATA_DIR, TRAIN_FILE_NAME))\n",
    "VALID_FILE_NAME = \"movielens_\" + MOVIELENS_DATA_SIZE + \"_valid.pkl\"\n",
    "valid.to_pickle(os.path.join(DATA_DIR, VALID_FILE_NAME))\n",
    "\n",
    "# Note, all the files under DATA_DIR will be uploaded to the data store\n",
    "ds = ws.get_default_datastore()\n",
    "ds.upload(\n",
    "    src_dir=DATA_DIR,\n",
    "    target_path='data',\n",
    "    overwrite=True,\n",
    "    show_progress=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Prepare Training Scripts\n",
    "Next step is to prepare scripts that AzureML Hyperdrive will use to train and evaluate models with selected hyperparameters. We re-use our [Wide-Deep Quickstart notebook](../00_quick_start/wide_deep_movielens.ipynb) for that. To run the model notebook from the Hyperdrive Run, all we need is to prepare an [entry script](../../reco_utils/aml/wide_deep.py) which parses the hyperparameter arguments, passes them to the notebook, and records the results of the notebook to AzureML Run logs by using `papermill`. Hyperdrive uses the logs to track the performance of each hyperparameter-set and finds the best performed one.  \n",
    "\n",
    "Here is a code snippet from the [entry script](../../reco_utils/aml/wide_deep.py):\n",
    "```\n",
    "import argparse\n",
    "import papermill as pm\n",
    "from azureml.core import Run\n",
    "run = Run.get_context()\n",
    "...\n",
    "parser = argparse.ArgumentParser()\n",
    "...\n",
    "parser.add_argument('--dnn-optimizer', type=str, dest='dnn_optimizer', ...\n",
    "parser.add_argument('--dnn-optimizer-lr', type=float, dest='dnn_optimizer_lr', ...\n",
    "...\n",
    "pm.execute_notebook(\n",
    "    \"../../notebooks/00_quick_start/wide_deep_movielens.ipynb\",\n",
    "    OUTPUT_NOTEBOOK,\n",
    "    parameters=params,\n",
    "    kernel_name='python3',\n",
    ")\n",
    "...\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare all the necessary scripts which will be loaded to our Hyperdrive Experiment Run\n",
    "SCRIPT_DIR = 'aml_script'\n",
    "\n",
    "# Clean-up scripts if already exists\n",
    "shutil.rmtree(SCRIPT_DIR, ignore_errors=True)\n",
    "\n",
    "# Copy scripts to SCRIPT_DIR temporarly\n",
    "shutil.copytree(os.path.join('..', '..', 'reco_utils'), os.path.join(SCRIPT_DIR, 'reco_utils'))\n",
    "\n",
    "# We re-use our model notebook for training and testing models.\n",
    "model_notebook_dir = os.path.join('notebooks', '00_quick_start')\n",
    "dest_model_notebook_dir = os.path.join(SCRIPT_DIR, model_notebook_dir)\n",
    "os.makedirs(dest_model_notebook_dir , exist_ok=True)\n",
    "shutil.copy(\n",
    "    os.path.join('..', '..', model_notebook_dir, 'wide_deep_movielens.ipynb'),\n",
    "    dest_model_notebook_dir\n",
    ")\n",
    "\n",
    "# This is our entry script for Hyperdrive Run\n",
    "ENTRY_SCRIPT_NAME = 'reco_utils/aml/wide_deep.py'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Setup and Run Hyperdrive Experiment\n",
    "[Hyperdrive](https://docs.microsoft.com/en-us/azure/machine-learning/service/how-to-tune-hyperparameters) create a machine learning Experiment [Run](https://docs.microsoft.com/en-us/python/api/azureml-core/azureml.core.run?view=azure-ml-py) on the workspace and utilizes child-runs to search the best set of hyperparameters.\n",
    "\n",
    "<br>\n",
    "\n",
    "#### 5.1 Create Experiment \n",
    "[Experiment](https://docs.microsoft.com/en-us/python/api/azureml-core/azureml.core.experiment(class)?view=azure-ml-py) is the main entry point into experimenting with AzureML. To create new Experiment or get the existing one, we pass our experimentation name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an experiment to track the runs in the workspace\n",
    "EXP_NAME = \"movielens_\" + MOVIELENS_DATA_SIZE + \"_wide_deep_model\"\n",
    "exp = aml.core.Experiment(workspace=ws, name=EXP_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.2 Define Search Space \n",
    "Now we define the search space of hyperparameters. For example, if you want to test different batch sizes of {64, 128, 256}, you can use `azureml.train.hyperdrive.choice(64, 128, 256)`. To search from a continuous space, use `uniform(start, end)`. For more options, see [Hyperdrive parameter expressions](https://docs.microsoft.com/en-us/python/api/azureml-train-core/azureml.train.hyperdrive.parameter_expressions?view=azure-ml-py).\n",
    "In this notebook, we fix model type as `wide_deep` and the number of epochs to 50.\n",
    "\n",
    "In the search space, we set different linear and DNN optimizers, structures, learning rates and regularization rates. Details about the hyperparameters can be found from our [Wide-Deep Quickstart notebook](../00_quick_start/wide_deep_movielens.ipynb).\n",
    "\n",
    "> Hyperdrive provides three different parameter sampling methods: `RandomParameterSampling`, `GridParameterSampling`, and `BayesianParameterSampling`. Details about each method can be found from [Azure doc](https://docs.microsoft.com/en-us/azure/machine-learning/service/how-to-tune-hyperparameters). Here, we use the Bayesian sampling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fixed parameters\n",
    "script_params = {\n",
    "    '--datastore': ds.as_mount(),\n",
    "    '--train-datapath': \"data/\" + TRAIN_FILE_NAME,\n",
    "    '--test-datapath': \"data/\" + VALID_FILE_NAME,\n",
    "    '--top-k': TOP_K,\n",
    "    '--user-col': USER_COL,\n",
    "    '--item-col': ITEM_COL,\n",
    "    '--item-feat-col': ITEM_FEAT_COL,\n",
    "    '--rating-col': RATING_COL,\n",
    "    '--metrics': METRICS,\n",
    "    '--epochs': EPOCHS,\n",
    "    '--model-type': 'wide_deep'\n",
    "}\n",
    "\n",
    "# Hyperparameter search space\n",
    "params = {\n",
    "    '--batch-size': hd.choice(64, 128, 256),\n",
    "    # Linear model hyperparameters\n",
    "    '--linear-optimizer': hd.choice('Ftrl'),  # 'SGD' easily got exploded loss in regression problems.\n",
    "    '--linear-optimizer-lr': hd.uniform(0.0001, 0.1),\n",
    "    '--linear-l1-reg': hd.uniform(0.0, 0.1),\n",
    "    # Deep model hyperparameters\n",
    "    '--dnn-optimizer': hd.choice('Adagrad', 'Adam'),\n",
    "    '--dnn-optimizer-lr': hd.uniform(0.0001, 0.1),\n",
    "    '--dnn-user-embedding-dim': hd.choice(4, 8, 16, 32, 64),\n",
    "    '--dnn-item-embedding-dim': hd.choice(4, 8, 16, 32, 64),\n",
    "    '--dnn-hidden-layer-1': hd.choice(0, 32, 64, 128, 256, 512, 1024),  # 0: not using this layer\n",
    "    '--dnn-hidden-layer-2': hd.choice(0, 32, 64, 128, 256, 512, 1024),\n",
    "    '--dnn-hidden-layer-3': hd.choice(0, 32, 64, 128, 256, 512, 1024),\n",
    "    '--dnn-hidden-layer-4': hd.choice(32, 64, 128, 256, 512, 1024),\n",
    "    '--dnn-batch-norm': hd.choice(0, 1),\n",
    "    '--dnn-dropout': hd.choice(0.0, 0.1, 0.2, 0.3, 0.4)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**AzureML Estimator** is the building block for training. An Estimator encapsulates the training code and parameters, the compute resources and runtime environment for a particular training scenario (Note, this is not TensorFlow's Estimator)\n",
    "\n",
    "We create one for our experimentation with the dependencies our model requires as follows:\n",
    "```\n",
    "conda_packages=['pandas', 'scikit-learn', 'tensorflow-gpu=1.12'],\n",
    "pip_packages=['ipykernel', 'papermill']\n",
    "```\n",
    "\n",
    "To the Hyperdrive Run Config, we set our primary metric name and the goal (our hyperparameter search criteria), hyperparameter sampling method, and number of total child-runs. The bigger the search space, the more number of runs we will need for better results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "est = aml.train.estimator.Estimator(\n",
    "    source_directory=SCRIPT_DIR,\n",
    "    entry_script=ENTRY_SCRIPT_NAME,\n",
    "    script_params=script_params,\n",
    "    compute_target=compute_target,\n",
    "    use_gpu=True,\n",
    "    conda_packages=['pandas', 'scikit-learn', 'tensorflow-gpu=1.12'],\n",
    "    pip_packages=['ipykernel', 'papermill']\n",
    ")\n",
    "\n",
    "hd_run_config = hd.HyperDriveRunConfig(\n",
    "    estimator=est, \n",
    "    hyperparameter_sampling=hd.BayesianParameterSampling(params),\n",
    "    primary_metric_name=METRICS[0],\n",
    "    primary_metric_goal=hd.PrimaryMetricGoal.MINIMIZE, \n",
    "    max_total_runs=MAX_TOTAL_RUNS,\n",
    "    max_concurrent_runs=MAX_CONCURRENT_RUNS\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.3 Run Experiment\n",
    "\n",
    "Now we submit the Run to our experiment. You can see the experiment progress from this notebook by using `azureml.widgets.RunDetails(hd_run).show()` or check from the Azure portal with the url link you can get by running `hd_run.get_portal_url()`.\n",
    "\n",
    "[]()|[]()\n",
    ":---:|:---:\n",
    "![](./images/aml_0.png)|![](./images/aml_1.png)\n",
    "\n",
    "<br>\n",
    "\n",
    "To load an existing Hyperdrive Run, use `hd_run = hd.HyperDriveRun(exp, <user-run-id>, hyperdrive_run_config=hd_run_config)`. You also can cancel the Run with `hd_run.cancel()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b30a8f3e4f34400beee84d5011f0d33",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "_HyperDriveWidget(widget_settings={'childWidgetDisplay': 'popup', 'send_telemetry': False, 'log_level': 'INFO'â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "hd_run = exp.submit(config=hd_run_config)\n",
    "widgets.RunDetails(hd_run).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "Once all the child-runs are finished, we can get the best run and the metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get best run and printout metrics\n",
    "best_run = hd_run.get_best_run_by_primary_metric()\n",
    "best_run_metrics = best_run.get_metrics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Best Run Id: movielens_100k_wide_deep_model_1549920424915_42\n",
      "\n",
      "* Best hyperparameters:\n",
      "Model type = wide_deep\n",
      "Batch size = 256.0\n",
      "Linear optimizer = Ftrl\n",
      "\tLearning rate = 0.0001\n",
      "\tL1 regularization = 0.0000\n",
      "DNN optimizer = Adagrad\n",
      "\tUser embedding dimension = 64.0\n",
      "\tItem embedding dimension = 4.0\n",
      "\tHidden units = [1024.0, 32.0, 1024.0]\n",
      "\tLearning rate = 0.1000\n",
      "\tDropout rate = 0.3000\n",
      "\tBatch normalization = 1.0\n",
      "\n",
      "* Performance metrics:\n",
      "\trmse = 0.9489\n",
      "\tmae = 0.7514\n",
      "\tndcg = 0.0542\n",
      "\tprecision = 0.0464\n"
     ]
    }
   ],
   "source": [
    "print(\"* Best Run Id:\", best_run.id)\n",
    "print(\"\\n* Best hyperparameters:\")\n",
    "print(\"Model type =\", best_run_metrics['MODEL_TYPE'])\n",
    "print(\"Batch size =\", best_run_metrics['BATCH_SIZE'])\n",
    "print(\"Linear optimizer =\", best_run_metrics['LINEAR_OPTIMIZER'])\n",
    "print(\"\\tLearning rate = {0:.4f}\".format(best_run_metrics['LINEAR_OPTIMIZER_LR']))\n",
    "print(\"\\tL1 regularization = {0:.4f}\".format(best_run_metrics['LINEAR_L1_REG']))\n",
    "print(\"DNN optimizer =\", best_run_metrics['DNN_OPTIMIZER'])\n",
    "print(\"\\tUser embedding dimension =\", best_run_metrics['DNN_USER_DIM'])\n",
    "print(\"\\tItem embedding dimension =\", best_run_metrics['DNN_ITEM_DIM'])\n",
    "hidden_units = []\n",
    "for i in range(1, 5):\n",
    "    hidden_nodes = best_run_metrics['DNN_HIDDEN_LAYER_{}'.format(i)]\n",
    "    if hidden_nodes > 0:\n",
    "        hidden_units.append(hidden_nodes)\n",
    "print(\"\\tHidden units =\", hidden_units)\n",
    "print(\"\\tLearning rate = {0:.4f}\".format(best_run_metrics['DNN_OPTIMIZER_LR']))\n",
    "print(\"\\tDropout rate = {0:.4f}\".format(best_run_metrics['DNN_DROPOUT']))\n",
    "print(\"\\tBatch normalization =\", best_run_metrics['DNN_BATCH_NORM'])\n",
    "# Metrics evaluated on validation set\n",
    "print(\"\\n* Performance metrics:\")\n",
    "for m in METRICS:\n",
    "    if m in RANKING_METRICS:\n",
    "        print(\"\\t{0} = {1:.4f}\".format(m, best_run_metrics['{}@{}'.format(m, TOP_K)]))\n",
    "    elif m in RATING_METRICS:\n",
    "        print(\"\\t{0} = {1:.4f}\".format(m, best_run_metrics[m]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Model Import and Test\n",
    "\n",
    "[Wide-Deep Quickstart notebook](../00_quick_start/wide_deep_movielens.ipynb), which we've used in our Hyperdrive Experiment, exports the trained model to the output folder (the output path is recorded at `best_run_metrics['saved_model_dir']`). We can download a model from the best run and test it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outputs/model/1549923805/\n",
      "Downloading outputs/model/1549923805/saved_model.pb..\n",
      "Downloading outputs/model/1549923805/variables/variables.data-00000-of-00002..\n",
      "Downloading outputs/model/1549923805/variables/variables.data-00001-of-00002..\n",
      "Downloading outputs/model/1549923805/variables/variables.index..\n",
      "INFO:tensorflow:Using default config.\n",
      "WARNING:tensorflow:Using temporary folder as model directory: /tmp/tmp244t3geh\n",
      "INFO:tensorflow:Using config: {'_model_dir': '/tmp/tmp244t3geh', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': allow_soft_placement: true\n",
      "graph_options {\n",
      "  rewrite_options {\n",
      "    meta_optimizer_iterations: ONE\n",
      "  }\n",
      "}\n",
      ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f3850892e48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n",
      "INFO:tensorflow:Checking available modes for SavedModelEstimator.\n",
      "INFO:tensorflow:Available modes for Estimator: ['train', 'eval', 'infer']\n"
     ]
    }
   ],
   "source": [
    "MODEL_DIR = 'aml_model'\n",
    "\n",
    "os.makedirs(MODEL_DIR, exist_ok=True)\n",
    "model_file_dir = os.path.normpath(best_run_metrics['saved_model_dir'][2:-1]) + '/'\n",
    "print(model_file_dir)\n",
    "for f in best_run.get_file_names():\n",
    "    if f.startswith(model_file_dir):\n",
    "        output_file_path = os.path.join(MODEL_DIR, f[len(model_file_dir):])\n",
    "        print(\"Downloading {}..\".format(f))\n",
    "        best_run.download_file(name=f, output_file_path=output_file_path)\n",
    "    \n",
    "saved_model = tf.contrib.estimator.SavedModelEstimator(MODEL_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = {\n",
    "    'col_user': USER_COL,\n",
    "    'col_item': ITEM_COL,\n",
    "    'col_rating': RATING_COL,\n",
    "    'col_prediction': 'prediction'\n",
    "}\n",
    "\n",
    "tf.logging.set_verbosity(tf.logging.WARN)\n",
    "\n",
    "# Prediction input function for TensorFlow SavedModel\n",
    "def predict_input_fn(df):\n",
    "    def input_fn():\n",
    "        examples = [None] * len(df)\n",
    "        for index, test_sample in df.iterrows():\n",
    "            example = tf.train.Example()\n",
    "\n",
    "            example.features.feature[USER_COL].int64_list.value.extend([test_sample[USER_COL]])\n",
    "            example.features.feature[ITEM_COL].int64_list.value.extend([test_sample[ITEM_COL]])\n",
    "            example.features.feature[ITEM_FEAT_COL].float_list.value.extend(test_sample[ITEM_FEAT_COL])\n",
    "\n",
    "            examples[index] = example.SerializeToString()\n",
    "        return {'inputs': tf.constant(examples)}\n",
    "    return input_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Input graph does not use tf.data.Dataset or contain a QueueRunner. That means predict yields forever. This is probably a mistake.\n",
      "count    25000.000000\n",
      "mean         3.552231\n",
      "std          0.623859\n",
      "min          1.125053\n",
      "25%          3.159035\n",
      "50%          3.670297\n",
      "75%          4.026501\n",
      "max          5.352178\n",
      "Name: prediction, dtype: float64 \n",
      "\n",
      "rmse = 0.9518256502703581\n",
      "mae = 0.7517491664409638\n",
      "rsquared = 0.28652283623980745\n",
      "exp_var = 0.28689955650085763\n"
     ]
    }
   ],
   "source": [
    "# Rating prediction set\n",
    "X_test = test.drop(RATING_COL, axis=1)\n",
    "X_test.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Rating prediction\n",
    "predictions = list(itertools.islice(\n",
    "    saved_model.predict(predict_input_fn(X_test)),\n",
    "    len(X_test)\n",
    "))\n",
    "\n",
    "prediction_df = X_test.copy()\n",
    "prediction_df['prediction'] = [p['outputs'][0] for p in predictions]\n",
    "print(prediction_df['prediction'].describe(), \"\\n\")\n",
    "for m, fn in RATING_METRICS.items():\n",
    "    result = fn(test, prediction_df, **cols)\n",
    "    print(m, \"=\", result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unique items\n",
    "if ITEM_FEAT_COL is None:\n",
    "    items = data.drop_duplicates(ITEM_COL)[[ITEM_COL]].reset_index(drop=True)\n",
    "else:\n",
    "    items = data.drop_duplicates(ITEM_COL)[[ITEM_COL, ITEM_FEAT_COL]].reset_index(drop=True)\n",
    "# Unique users\n",
    "users = data.drop_duplicates(USER_COL)[[USER_COL]].reset_index(drop=True)\n",
    "\n",
    "# Ranking prediction set\n",
    "ranking_pool = user_item_pairs(\n",
    "    user_df=users,\n",
    "    item_df=items,\n",
    "    user_col=USER_COL,\n",
    "    item_col=ITEM_COL,\n",
    "    user_item_filter_df=pd.concat([train, valid]),  # remove seen items\n",
    "    shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "map@10 = 0.009923755277595944\n",
      "ndcg@10 = 0.07203458393278257\n",
      "precision@10 = 0.06167728237791933\n",
      "recall@10 = 0.023045098050417438\n"
     ]
    }
   ],
   "source": [
    "predictions = []\n",
    "for pool in np.array_split(ranking_pool, 10):\n",
    "    pool.reset_index(drop=True, inplace=True)\n",
    "    # Rating prediction\n",
    "    pred = list(itertools.islice(\n",
    "        saved_model.predict(predict_input_fn(pool)),\n",
    "        len(pool)\n",
    "    ))\n",
    "    predictions.extend([p['outputs'][0] for p in pred])\n",
    "    \n",
    "ranking_pool['prediction'] = predictions\n",
    "\n",
    "for m, fn in RANKING_METRICS.items():\n",
    "    result = fn(test, ranking_pool, **{**cols, 'k': TOP_K})\n",
    "    name = \"{}@{}\".format(m, TOP_K)\n",
    "    print(name, \"=\", result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span id=\"google-wide-deep-baseline\">Wide-and-Deep Baseline</span>\n",
    "To see if Hyperdrive found good hyperparameters, we simply compare with the model with known hyperparameters from [TensorFlow's wide-deep learning example](https://github.com/tensorflow/models/tree/master/official/wide_deep) which uses only the DNN part of wide-and-deep learning for [MovieLens] data.(https://github.com/tensorflow/models/blob/master/official/wide_deep/movielens_main.py).\n",
    "\n",
    "> Note, this is not 'apples to apples' comparison. For example, TensorFlow's movielens example uses *rating-timestamp* as a numeric feature, but we did not use that here because we think the timestamps are not relevant to the movies' ratings. This comparison is more like to show how Hyperdrive can help to find comparable hyperparameters without requiring exhaustive efforts in searching through a huge space. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "70f92f1448314beb9e4449b699a5ef70",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=30), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training and evaluation of Wide-and-Deep model took 163.8261215686798 secs.\n"
     ]
    }
   ],
   "source": [
    "OUTPUT_NOTEBOOK = \"output.ipynb\"\n",
    "OUTPUT_MODEL_DIR = \"known_hyperparam_model_checkpoints\"\n",
    "params = {\n",
    "    'MOVIELENS_DATA_SIZE': MOVIELENS_DATA_SIZE,\n",
    "    'TOP_K': TOP_K,\n",
    "    'MODEL_TYPE': 'deep',\n",
    "    'EPOCHS': EPOCHS,\n",
    "    'BATCH_SIZE': 256,\n",
    "    'DNN_OPTIMIZER': 'Adam',\n",
    "    'DNN_OPTIMIZER_LR': 0.001,\n",
    "    'DNN_HIDDEN_LAYER_1': 256,\n",
    "    'DNN_HIDDEN_LAYER_2': 256,\n",
    "    'DNN_HIDDEN_LAYER_3': 256,\n",
    "    'DNN_HIDDEN_LAYER_4': 128,\n",
    "    'DNN_USER_DIM': 16,\n",
    "    'DNN_ITEM_DIM': 64,\n",
    "    'DNN_DROPOUT': 0.3,\n",
    "    'DNN_BATCH_NORM': 0,\n",
    "    'MODEL_DIR': OUTPUT_MODEL_DIR,\n",
    "    'EVALUATE_WHILE_TRAINING': False,\n",
    "    'EXPORT_DIR_BASE': OUTPUT_MODEL_DIR,\n",
    "    'METRICS': METRICS\n",
    "}\n",
    "\n",
    "start_time = time.time()\n",
    "pm.execute_notebook(\n",
    "    \"../00_quick_start/wide_deep_model_movielens.ipynb\",\n",
    "    OUTPUT_NOTEBOOK,\n",
    "    parameters=params,\n",
    "    kernel_name='python3'\n",
    ")\n",
    "end_time = time.time()\n",
    "print(\"Training and evaluation of Wide-and-Deep model took\", end_time-start_time, \"secs.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rmse = 1.0100459972074924\n",
      "mae = 0.8004435630893707\n",
      "ndcg@10 = 0.013171907282473755\n",
      "precision@10 = 0.01528662420382166\n"
     ]
    }
   ],
   "source": [
    "nb = pm.read_notebook(OUTPUT_NOTEBOOK)\n",
    "for m in METRICS:\n",
    "    if m in RANKING_METRICS:\n",
    "        key = '{}@{}'.format(m, TOP_K)\n",
    "        print(key, \"=\", nb.data[key])\n",
    "    elif m in RATING_METRICS:\n",
    "        print(m, \"=\", nb.data[m])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Concluding Remark\n",
    "We showed how to tune hyperparameters by utilizing Azure Machine Learning service. Complex and powerful models like Wide-and-Deep model often have many number of hyperparameters that affect on the recommendation accuracy, and it is not practical to tune the model without using a GPU cluster. For example, a training and evaluation of a model took around 3 minutes on 100k MovieLens data on a single *Standard NC6* VM as we tested from the [above cell](#google-wide-deep-baseline). When we used 1M MovieLens, it took about 47 minutes. If we want to investigate through 100 different combinations of hyperparameters **manually**, it will take **78 hours** on the VM and we may still wonder if we had tested good candidates of hyperparameters. With AzureML, as we shown in this notebook, we can easily setup different size of GPU cluster fits to our problem and utilize Bayesian sampling to navigate through the huge search space efficiently, and tweak the experiment with different criteria and algorithms for further research."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "shutil.rmtree(SCRIPT_DIR, ignore_errors=True)\n",
    "shutil.rmtree(DATA_DIR, ignore_errors=True)\n",
    "shutil.rmtree(MODEL_DIR, ignore_errors=True)\n",
    "\n",
    "os.remove(OUTPUT_NOTEBOOK)\n",
    "shutil.rmtree(OUTPUT_MODEL_DIR, ignore_errors=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
