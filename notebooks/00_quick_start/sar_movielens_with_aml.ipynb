{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<i>Copyright (c) Microsoft Corporation. All rights reserved.</i>\n",
    "\n",
    "<i>Licensed under the MIT License.</i>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train SAR Single Node on MovieLens with Azure Machine Learning (Python, CPU)\n",
    "\n",
    "This notebook provides an exmaple of how to train SAR on remote compute resources. Details of SAR algorithm can be found in [SAR Python CPU Movielens](https://github.com/Microsoft/Recommenders/blob/master/notebooks/00_quick_start/sar_movielens.ipynb) notebook. \n",
    "\n",
    "This notebook showcases what are the changes required to run local script on AzureML targets. AzureML comes with auto-scaling and gpu options and can greatly accelerate model training. There're a few more notebooks in `04_model_select_and_optimize` folder that shows how to use AzureML to do hyperparameter tuning. \n",
    "\n",
    "### Prerequisite\n",
    "This notebook requires an AzureML workspace to be set up. Please follow [this notebook](https://github.com/Azure/MachineLearningNotebooks/blob/master/configuration.ipynb) on how to create an AzureML workspace."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0 Setup environment\n",
    "### Get workspace and create experiment\n",
    "The workspace was created in configuration notebook and can be loaded using `Workspace.from_config()`.\n",
    "\n",
    "The DSVM created in configuration notebook is called `cpucluster`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found the config file in: /data/home/cathack/notebooks/Recommenders/notebooks/aml_config/config.json\n",
      "setup-ws\n",
      "jingywa-test\n",
      "eastus2\n",
      "b8c23406-f9b5-4ccb-8a65-a8cb5dcd6a5a\n"
     ]
    }
   ],
   "source": [
    "# set the environment path to find Recommenders\n",
    "import sys\n",
    "sys.path.append(\"../../\")\n",
    "from reco_utils.dataset import movielens\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "import azureml\n",
    "from azureml.core import Workspace, Run, Experiment\n",
    "from azureml.train.estimator import Estimator\n",
    "from azureml.widgets import RunDetails\n",
    "\n",
    "# load workspace configuration from the config.json file in the current folder.\n",
    "ws = Workspace.from_config()\n",
    "print(ws.name, ws.resource_group, ws.location, ws.subscription_id, sep = '\\n')\n",
    "# create experiment\n",
    "experiment_name = 'movielens-sar'\n",
    "exp = Experiment(workspace=ws, name=experiment_name)\n",
    "# attach amlcompute\n",
    "compute_target = ws.compute_targets[\"cpucluster\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# top k items to recommend\n",
    "TOP_K = 10\n",
    "\n",
    "# Select Movielens data size: 100k, 1m, 10m, or 20m\n",
    "MOVIELENS_DATA_SIZE = '100k'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download dataset and upload to data store\n",
    "Now make the data accessible remotely by uploading that data from your local machine into Azure so it can be accessed for remote training. The datastore is a convenient construct associated with your workspace for you to upload/download data, and interact with it from your remote compute targets. It is backed by Azure blob storage account.\n",
    "\n",
    "The data files are uploaded into a directory named `data` at the root of the datastore."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/data/home/cathack/notebooks/Recommenders/notebooks/00_quick_start/data\n",
      "AzureBlob setupwsstoragewzlfyzlr azureml-blobstore-427c91f1-92ba-45a5-b01e-4e923a230fcc\n"
     ]
    }
   ],
   "source": [
    "# download dataset\n",
    "os.makedirs('./data', exist_ok = True)\n",
    "\n",
    "datapath, item_datapath = movielens.download_datafile(\n",
    "    size=MOVIELENS_DATA_SIZE,\n",
    "    local_cache_path='./data/ml.zip'\n",
    ")\n",
    "# print (datapath, item_datapath)\n",
    "\n",
    "# upload to data store\n",
    "ds = ws.get_default_datastore()\n",
    "print(ds.datastore_type, ds.account_name, ds.container_name)\n",
    "ds.upload(src_dir='./data', target_path='movielens', overwrite=True, show_progress=True)\n",
    "\n",
    "# clean up\n",
    "movielens._clean_up(datapath)\n",
    "movielens._clean_up(item_datapath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 Train on remote cluster\n",
    "### Create a directory\n",
    "Create a directory to deliver the necessary code from your computer to the remote resource."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "script_folder = './movielens-sar'\n",
    "os.makedirs(script_folder, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a training script\n",
    "To submit the job to the cluster, first create a training script. Run the following code to create the training script called `train.py` in the directory you just created. This training adds a regularization rate to the training algorithm, so produces a slightly different model than the local version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing ./movielens-sar/train.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile $script_folder/train.py\n",
    "\n",
    "import argparse\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import papermill as pm\n",
    "import itertools\n",
    "import logging\n",
    "import time\n",
    "\n",
    "from azureml.core import Run\n",
    "from sklearn.externals import joblib\n",
    "\n",
    "from reco_utils.dataset import movielens\n",
    "from reco_utils.dataset.python_splitters import python_random_split\n",
    "from reco_utils.evaluation.python_evaluation import map_at_k, ndcg_at_k, precision_at_k, recall_at_k\n",
    "from reco_utils.recommender.sar.sar_singlenode import SARSingleNode\n",
    "\n",
    "# get hold of the current run\n",
    "run = Run.get_context()\n",
    "\n",
    "# let user feed in 2 parameters, the location of the data files (from datastore), and the regularization rate of the logistic regression model\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--data-folder', type=str, dest='data_folder', help='data folder mounting point')\n",
    "parser.add_argument('--top-k', type=int, dest='top_k', default=10, help='top k items to recommend')\n",
    "parser.add_argument('--data-size', type=str, dest='data_size', default=10, help='Movielens data size: 100k, 1m, 10m, or 20m')\n",
    "args = parser.parse_args()\n",
    "\n",
    "data_folder = os.path.join(args.data_folder, 'movielens')\n",
    "print('Data folder:', data_folder)\n",
    "\n",
    "# load data into pandas data frame\n",
    "data = movielens.load_pandas_df_from_ds(\n",
    "    size=args.data_size,\n",
    "    header=['UserId','MovieId','Rating','Timestamp'],\n",
    "    ds_path=os.path.join(data_folder, 'u.data'))\n",
    "\n",
    "# Convert the float precision to 32-bit in order to reduce memory consumption \n",
    "data.loc[:, 'Rating'] = data['Rating'].astype(np.float32)\n",
    "\n",
    "data.head()\n",
    "\n",
    "train, test = python_random_split(data)\n",
    "\n",
    "# instantiate the SAR algorithm and set the index\n",
    "header = {\n",
    "    \"col_user\": \"UserId\",\n",
    "    \"col_item\": \"MovieId\",\n",
    "    \"col_rating\": \"Rating\",\n",
    "    \"col_timestamp\": \"Timestamp\",\n",
    "}\n",
    "\n",
    "logging.basicConfig(level=logging.DEBUG, \n",
    "                    format='%(asctime)s %(levelname)-8s %(message)s')\n",
    "\n",
    "model = SARSingleNode(\n",
    "    remove_seen=True, similarity_type=\"jaccard\", \n",
    "    time_decay_coefficient=30, time_now=None, timedecay_formula=True, **header\n",
    ")\n",
    "\n",
    "# train the SAR model\n",
    "start_time = time.time()\n",
    "\n",
    "model.fit(train)\n",
    "\n",
    "train_time = time.time() - start_time\n",
    "run.log(name=\"Training time\", value=\"Took {} seconds for training.\".format(train_time))\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "top_k = model.recommend_k_items(test)\n",
    "\n",
    "test_time = time.time() - start_time\n",
    "run.log(name=\"Prediction time\", value=\"Took {} seconds for prediction.\".format(test_time))\n",
    "\n",
    "# TODO: remove this call when the model returns same type as input\n",
    "top_k['UserId'] = pd.to_numeric(top_k['UserId'])\n",
    "top_k['MovieId'] = pd.to_numeric(top_k['MovieId'])\n",
    "\n",
    "# evaluate\n",
    "eval_map = map_at_k(test, top_k, col_user=\"UserId\", col_item=\"MovieId\", \n",
    "                    col_rating=\"Rating\", col_prediction=\"prediction\", \n",
    "                    relevancy_method=\"top_k\", k=args.top_k)\n",
    "eval_ndcg = ndcg_at_k(test, top_k, col_user=\"UserId\", col_item=\"MovieId\", \n",
    "                      col_rating=\"Rating\", col_prediction=\"prediction\", \n",
    "                      relevancy_method=\"top_k\", k=args.top_k)\n",
    "eval_precision = precision_at_k(test, top_k, col_user=\"UserId\", col_item=\"MovieId\", \n",
    "                                col_rating=\"Rating\", col_prediction=\"prediction\", \n",
    "                                relevancy_method=\"top_k\", k=args.top_k)\n",
    "eval_recall = recall_at_k(test, top_k, col_user=\"UserId\", col_item=\"MovieId\", \n",
    "                          col_rating=\"Rating\", col_prediction=\"prediction\", \n",
    "                          relevancy_method=\"top_k\", k=args.top_k)\n",
    "\n",
    "pm.record(\"map\", eval_map)\n",
    "pm.record(\"ndcg\", eval_ndcg)\n",
    "pm.record(\"precision\", eval_precision)\n",
    "pm.record(\"recall\", eval_recall)\n",
    "pm.record(\"train_time\", train_time)\n",
    "pm.record(\"test_time\", test_time)\n",
    "\n",
    "os.makedirs('outputs', exist_ok=True)\n",
    "# note file saved in the outputs folder is automatically uploaded into experiment record\n",
    "joblib.dump(value=model, filename='outputs/movielens_sar_model.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./movielens-sar/reco_utils/'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shutil.rmtree('./movielens-sar/reco_utils/', ignore_errors=True)\n",
    "shutil.copytree('../../reco_utils/', './movielens-sar/reco_utils/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create an estimator\n",
    "An estimator object is used to submit the run.  Create your estimator by running the following code to define:\n",
    "\n",
    "* The name of the estimator object, `est`\n",
    "* The directory that contains your scripts. All the files in this directory are uploaded into the cluster nodes for execution. \n",
    "* The compute target.  In this case you will use the AmlCompute you created\n",
    "* The training script name, train.py\n",
    "* Parameters required from the training script \n",
    "* Python packages needed for training\n",
    "\n",
    "In this tutorial, this target is AmlCompute. All files in the script folder are uploaded into the cluster nodes for execution. `ds.as_mount()` mounts a datastore on the remote compute and returns the folder. See documentation [here](https://docs.microsoft.com/en-us/azure/machine-learning/service/how-to-access-data#access-datastores-during-training)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": [
     "configure estimator"
    ]
   },
   "outputs": [],
   "source": [
    "script_params = {\n",
    "    '--data-folder': ds.as_mount(),\n",
    "    '--top-k': TOP_K,\n",
    "    '--data-size': MOVIELENS_DATA_SIZE\n",
    "}\n",
    "\n",
    "est = Estimator(source_directory=script_folder,\n",
    "                script_params=script_params,\n",
    "                compute_target=compute_target,\n",
    "                entry_script='train.py',\n",
    "                conda_packages=['pandas'],\n",
    "                pip_packages=['papermill', 'sklearn'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submit the job to the cluster\n",
    "Run the experiment by submitting the estimator object. You can check the status of current experiment in Azure Portal by clicking link below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table style=\"width:100%\"><tr><th>Experiment</th><th>Id</th><th>Type</th><th>Status</th><th>Details Page</th><th>Docs Page</th></tr><tr><td>movielens-sar</td><td>movielens-sar_1552614476887</td><td>azureml.scriptrun</td><td>Starting</td><td><a href=\"https://mlworkspace.azure.ai/portal/subscriptions/b8c23406-f9b5-4ccb-8a65-a8cb5dcd6a5a/resourceGroups/jingywa-test/providers/Microsoft.MachineLearningServices/workspaces/setup-ws/experiments/movielens-sar/runs/movielens-sar_1552614476887\" target=\"_blank\" rel=\"noopener\">Link to Azure Portal</a></td><td><a href=\"https://docs.microsoft.com/en-us/python/api/azureml-core/azureml.core.script_run.ScriptRun?view=azure-ml-py\" target=\"_blank\" rel=\"noopener\">Link to Documentation</a></td></tr></table>"
      ],
      "text/plain": [
       "Run(Experiment: movielens-sar,\n",
       "Id: movielens-sar_1552614476887,\n",
       "Type: azureml.scriptrun,\n",
       "Status: Starting)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run = exp.submit(config=est)\n",
    "run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 Monitor a remote run\n",
    "\n",
    "### Jupyter widget\n",
    "\n",
    "Alternatively, watch the progress of the run with a Jupyter widget.  Like the run submission, the widget is asynchronous and provides live updates every 10-15 seconds until the job completes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fced50bcd73d47d8b318357d7f4ca0a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "_UserRunWidget(widget_settings={'childWidgetDisplay': 'popup', 'send_telemetry': False, 'log_level': 'NOTSET',â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "RunDetails(run).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the run is complete, you can see files associated with that run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['azureml-logs/20_image_build_log.txt']\n"
     ]
    }
   ],
   "source": [
    "print(run.get_file_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "Python 3.6 - AzureML",
   "language": "python",
   "name": "python3-azureml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
