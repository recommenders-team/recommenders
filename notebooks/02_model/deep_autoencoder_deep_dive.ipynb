{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<i>Copyright (c) Microsoft Corporation. All rights reserved.</i>\n",
    "\n",
    "<i>Licensed under the MIT License.</i>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Autoencoder Deep Dive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OS: linux\n",
      "Python:  3.6.7 | packaged by conda-forge | (default, Nov 21 2018, 03:09:43) \n",
      "[GCC 7.3.0]\n",
      "PyTorch: 1.0.0\n",
      "Number of CPU processors: 6\n",
      "Number of GPUs: 1\n",
      "CUDA Version 9.2.148\n",
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"../../\")\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import copy\n",
    "\n",
    "from reco_utils.common.gpu_utils import get_number_gpus, get_cuda_version\n",
    "from reco_utils.common.general_utils import get_number_processors\n",
    "from reco_utils.dataset import movielens\n",
    "from reco_utils.dataset.python_splitters import python_chrono_split\n",
    "from reco_utils.evaluation.python_evaluation import rmse, mae, rsquared, exp_var\n",
    "from reco_utils.recommender.deep_autoencoder.autoencoder import AutoEncoder\n",
    "from reco_utils.recommender.deep_autoencoder.data import UserItemRecDataProvider\n",
    "from reco_utils.recommender.deep_autoencoder.utils import add_gpu, init_optimizer, MSEloss\n",
    "\n",
    "import logging\n",
    "log = logging.getLogger(__name__)\n",
    "\n",
    "print(\"OS:\", sys.platform)\n",
    "print(\"Python: \", sys.version)\n",
    "print(\"PyTorch:\", torch.__version__)\n",
    "print(\"Number of CPU processors:\", get_number_processors())\n",
    "print(\"Number of GPUs:\", get_number_gpus())\n",
    "print(get_cuda_version())\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = \"ratings_train.csv\"\n",
    "valid_path = \"ratings_valid.csv\"\n",
    "test_path = \"ratings_test.csv\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7fb7e70acf30>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_params = {'batch_size': 128,\n",
    "                'major': 'users',  # major position is the first column id of input data\n",
    "                'itemIdInd': 1,  # the second index is the items\n",
    "                'userIdInd': 0,  # the first index is the users/customers\n",
    "                'delimiter': ',',\n",
    "                'header': True,\n",
    "                \"src_file\": train_path\n",
    "                }\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_layer = UserItemRecDataProvider(params=data_params)\n",
    "#dir(data_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(indices=tensor([[   0,    0,    0,  ...,  127,  127,  127],\n",
      "                       [ 424,   54, 2049,  ..., 1430, 1439, 2230]]),\n",
      "       values=tensor([5., 3., 3.,  ..., 5., 5., 5.]),\n",
      "       size=(128, 6375), nnz=9835, layout=torch.sparse_coo)\n"
     ]
    }
   ],
   "source": [
    "for i, mb in enumerate(data_layer.iterate_one_epoch()):\n",
    "    print(mb)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_params = copy.deepcopy(data_params)\n",
    "eval_params['src_file'] = valid_path\n",
    "validation_layer = UserItemRecDataProvider(\n",
    "    params=eval_params,\n",
    "    user_id_map=data_layer.user_id_map,\n",
    "    item_id_map=data_layer.item_id_map)\n",
    "validation_layer.src_data = data_layer.data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../../reco_utils/recommender/deep_autoencoder/autoencoder.py:46: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
      "  weight_init.xavier_uniform(w)\n",
      "../../reco_utils/recommender/deep_autoencoder/autoencoder.py:61: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
      "  weight_init.xavier_uniform(w)\n"
     ]
    }
   ],
   "source": [
    "hidden_layers = [1024, 512, 512, 128]\n",
    "model = AutoEncoder(\n",
    "    layer_sizes=[data_layer.vector_dim] + hidden_layers,\n",
    "    nl_type=\"selu\",\n",
    "    is_constrained=False,\n",
    "    dp_drop_prob=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = add_gpu(model, \"0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer, scheduler = init_optimizer(model,\n",
    "                       optimization_method=\"momentum\",\n",
    "                       lr=0.005,\n",
    "                       wd=0.00001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.autograd import Variable\n",
    "from math import sqrt\n",
    "\n",
    "\n",
    "cuda_availability = True\n",
    "def train_loop(rencoder, optimizer, scheduler=None):    \n",
    "    \"\"\"\n",
    "    Internal train loop\n",
    "    \"\"\"\n",
    "    t_loss = 0.0\n",
    "    t_loss_denom = 0.0\n",
    "    global_step = 0\n",
    "    best_loss = sys.maxsize\n",
    "    best_epoch = 0\n",
    "    epoch = 0\n",
    "    losing_patience = 0\n",
    "\n",
    "    # Params\n",
    "    noise_prob = 0.0\n",
    "    num_epochs = 20\n",
    "    aug_step = 1\n",
    "\n",
    "\n",
    "    if noise_prob > 0.0:\n",
    "        dp = nn.Dropout(p=noise_prob)\n",
    "\n",
    "    # Train until finish epochs or early stoping fires\n",
    "    while epoch < num_epochs and losing_patience < 10:\n",
    "        print('Doing epoch {} of {}'.format(epoch, num_epochs))\n",
    "        rencoder.train()\n",
    "        total_epoch_loss = 0.0\n",
    "        denom = 0.0\n",
    "        if scheduler:\n",
    "            scheduler.step()\n",
    "        for i, mb in enumerate(data_layer.iterate_one_epoch()):\n",
    "            inputs = Variable(mb.cuda().to_dense()) if cuda_availability else Variable(mb.to_dense())\n",
    "            optimizer.zero_grad()\n",
    "            loss, outputs = _backprop(rencoder, inputs, optimizer)\n",
    "            global_step += 1\n",
    "            t_loss += loss.data.item()#loss.data[0]\n",
    "            t_loss_denom += 1\n",
    "            total_epoch_loss += loss.data.item()#loss.data[0]\n",
    "            denom += 1\n",
    "\n",
    "            if aug_step > 0:\n",
    "                # Magic data augmentation trick happen here\n",
    "                for t in range(aug_step):\n",
    "                    inputs = Variable(outputs.data)\n",
    "                    if noise_prob > 0.0:\n",
    "                        inputs = dp(inputs)\n",
    "                    optimizer.zero_grad()\n",
    "                    loss, outputs = _backprop(\n",
    "                        rencoder, inputs, optimizer)\n",
    "\n",
    "        # Track model with lowest loss\n",
    "        epoch_loss = sqrt(total_epoch_loss/denom)\n",
    "        print(\"Epoch {} - Training loss: {}\".format(epoch, epoch_loss))\n",
    "        if True:# self.params['use_validation']:\n",
    "            epoch_loss = _evaluate_on_validation_set(rencoder)\n",
    "            print(\"Epoch {} - Validation loss: {}\".format(epoch,\n",
    "                                                              epoch_loss))\n",
    "        if epoch_loss < best_loss:\n",
    "            losing_patience = 0\n",
    "            best_loss = epoch_loss\n",
    "            best_epoch = epoch\n",
    "            best_model_wts = copy.deepcopy(rencoder.state_dict())\n",
    "        else:\n",
    "            # early stoping\n",
    "            losing_patience += 1\n",
    "        epoch += 1\n",
    "\n",
    "    # Save final model\n",
    "    print(\"Best loss {} in epoch {}\".format(best_loss, best_epoch))\n",
    "    #self._save_model(best_model_wts, best_epoch)\n",
    "    #rencoder.load_state_dict(best_model_wts)\n",
    "\n",
    "def _backprop(rencoder, inputs, optimizer):\n",
    "    outputs = rencoder(inputs)\n",
    "    loss, num_ratings = MSEloss(outputs, inputs)\n",
    "    loss = loss / num_ratings\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss, outputs\n",
    "\n",
    "def _evaluate_on_validation_set(rencoder):\n",
    "    rencoder.eval()\n",
    "    denom = 0.0\n",
    "    total_epoch_loss = 0.0\n",
    "    for target_mb, user_profile in validation_layer.iterate_one_epoch_eval():\n",
    "        inputs = Variable(user_profile.cuda().to_dense()) if cuda_availability else Variable(user_profile.to_dense())\n",
    "        targets = Variable(target_mb.cuda().to_dense()) if cuda_availability else Variable(target_mb.to_dense())\n",
    "        outputs = rencoder(inputs)\n",
    "        loss, num_ratings = MSEloss(outputs, targets)\n",
    "        total_epoch_loss += loss.data.item()#loss.data[0]\n",
    "        denom += num_ratings.data.item()#num_ratings.data[0]\n",
    "    return sqrt(total_epoch_loss / denom)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doing epoch 0 of 20\n",
      "Epoch 0 - Training loss: 0.8397684036877522\n",
      "Epoch 0 - Validation loss: 1.0088240287902315\n",
      "Doing epoch 1 of 20\n",
      "Epoch 1 - Training loss: 0.8367286859629405\n",
      "Epoch 1 - Validation loss: 1.0082671629657438\n",
      "Doing epoch 2 of 20\n",
      "Epoch 2 - Training loss: 0.8378885122419276\n",
      "Epoch 2 - Validation loss: 1.007014279433431\n",
      "Doing epoch 3 of 20\n",
      "Epoch 3 - Training loss: 0.8360690931328426\n",
      "Epoch 3 - Validation loss: 1.005681530466913\n",
      "Doing epoch 4 of 20\n",
      "Epoch 4 - Training loss: 0.8298117032809726\n",
      "Epoch 4 - Validation loss: 1.0047572805108538\n",
      "Doing epoch 5 of 20\n",
      "Epoch 5 - Training loss: 0.828885582064895\n",
      "Epoch 5 - Validation loss: 1.0038280099692707\n",
      "Doing epoch 6 of 20\n",
      "Epoch 6 - Training loss: 0.8288188187116898\n",
      "Epoch 6 - Validation loss: 1.0027213862829305\n",
      "Doing epoch 7 of 20\n",
      "Epoch 7 - Training loss: 0.825830375487737\n",
      "Epoch 7 - Validation loss: 1.001512954053215\n",
      "Doing epoch 8 of 20\n",
      "Epoch 8 - Training loss: 0.8246885506528209\n",
      "Epoch 8 - Validation loss: 1.000410581692314\n",
      "Doing epoch 9 of 20\n",
      "Epoch 9 - Training loss: 0.8189405175943654\n",
      "Epoch 9 - Validation loss: 0.9999826142435578\n",
      "Doing epoch 10 of 20\n",
      "Epoch 10 - Training loss: 0.8185086644459183\n",
      "Epoch 10 - Validation loss: 0.9994990971470336\n",
      "Doing epoch 11 of 20\n",
      "Epoch 11 - Training loss: 0.8238109357115891\n",
      "Epoch 11 - Validation loss: 0.9989202198242709\n",
      "Doing epoch 12 of 20\n",
      "Epoch 12 - Training loss: 0.8260014856546818\n",
      "Epoch 12 - Validation loss: 0.9987298251024701\n",
      "Doing epoch 13 of 20\n",
      "Epoch 13 - Training loss: 0.8209925711710367\n",
      "Epoch 13 - Validation loss: 0.9982196218755716\n",
      "Doing epoch 14 of 20\n",
      "Epoch 14 - Training loss: 0.8151333609685844\n",
      "Epoch 14 - Validation loss: 0.9975937166983538\n",
      "Doing epoch 15 of 20\n",
      "Epoch 15 - Training loss: 0.8172146094294878\n",
      "Epoch 15 - Validation loss: 0.9968625121431043\n",
      "Doing epoch 16 of 20\n",
      "Epoch 16 - Training loss: 0.81629582438571\n",
      "Epoch 16 - Validation loss: 0.9964142042849181\n",
      "Doing epoch 17 of 20\n",
      "Epoch 17 - Training loss: 0.8131369734871394\n",
      "Epoch 17 - Validation loss: 0.9960070993605609\n",
      "Doing epoch 18 of 20\n",
      "Epoch 18 - Training loss: 0.8133916950262393\n",
      "Epoch 18 - Validation loss: 0.9954787681757146\n",
      "Doing epoch 19 of 20\n",
      "Epoch 19 - Training loss: 0.8161025582148054\n",
      "Epoch 19 - Validation loss: 0.9950957034046919\n",
      "Best loss 0.9950957034046919 in epoch 19\n"
     ]
    }
   ],
   "source": [
    "train_loop(model, optimizer, scheduler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (reco_gpu)",
   "language": "python",
   "name": "reco_gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
