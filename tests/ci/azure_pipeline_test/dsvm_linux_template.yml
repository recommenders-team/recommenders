# Copyright (c) Microsoft Corporation. All rights reserved.
# Licensed under the MIT License.

parameters:
- name: test_types
  type: object
  default: {}  # can include unit, smoke, and integration
- name: task_name
  type: string
- name: timeout
  type: number
  default: 20
- name: conda_env
  type: string
- name: conda_opts
  type: string
- name: pip_opts
  type: string
- name: pytest_markers
  type: string
- name: pytest_params
  type: string
  default: ""
- name: install
  type: string
  default: ""

jobs:
- job: "${{ parameters.conda_env }}" # Job name can't have spaces
  displayName: "${{ parameters.task_name }}"
  timeoutInMinutes: ${{ parameters.timeout }}
  pool:
    name: $(Agent_Pool)

  steps:
  - script: |
      echo "##vso[task.prependpath]/data/anaconda/bin"

      # Make sure there is no conda environment
      conda env remove -n ${{ parameters.conda_env }} || exit -1

      # Create a clean conda environment
      conda create -fyn ${{ parameters.conda_env }} ${{ parameters.conda_opts }}
      eval "$(conda shell.bash hook)"
      conda activate ${{ parameters.conda_env }}

      # install reco-utils and needed dependencies
      pip install pytest>=3.6.4 || exit -1
      if [ "${{ parameters.install }}" = "release" ]; then
        rm -rf dist
        echo "   --- BUILDING PACKAGE ---"
        pip install setuptools wheel twine || exit -1
        python setup.py sdist bdist_wheel || exit -1
        echo "   --- INSTALLING WHEEL ---"
        pip install --user dist/ms_recommenders*.whl${{ parameters.pip_opts }} --force-reinstall || exit -1
      else
        echo "Installing latest code"
        pip install .${{ parameters.pip_opts }} || exit -1
      fi
      
      # clean up any previous test results        
      rm -rf reports
    displayName: 'Install Dependencies'

  - ${{ each test in parameters.test_types }}:
    - script: |
        eval "$(conda shell.bash hook)"
        conda activate ${{ parameters.conda_env }}
        if [[ "${{ parameters.conda_env }}" == *"spark"* ]]; then
          export PYSPARK_PYTHON=`which python`
          export PYSPARK_DRIVER_PYTHON=`which python`
          export SPARK_DIR=$PWD/build/spark
          rm -rf $SPARK_DIR || exit -1
          mkdir -p $SPARK_DIR || exit -1
          wget https://mirrors.ukfast.co.uk/sites/ftp.apache.org/spark/spark-2.4.8/spark-2.4.8-bin-hadoop2.7.tgz -O $SPARK_DIR/spark.tgz || exit -1
          tar -xvzf $SPARK_DIR/spark.tgz -C $SPARK_DIR --strip-components=1 || exit -1
          SPARK_HOME=$SPARK_DIR
        fi
        
        if [ "${{ test }}" == "unit" ]; then
            export TEST_MARKER=""
        else
            export TEST_MARKER="${{ test }} and "
        fi

        # run tests
        pytest tests/${{ test }} \
          ${{ parameters.pytest_params }} \
          -m "${TEST_MARKER}${{ parameters.pytest_markers }}" \
          --durations 0 \
          --junitxml=reports/test-${{ test }}-${{ parameters.conda_env }}.xml || exit -1
        
        # rm -rf $SPARK_DIR || exit -1
        conda deactivate
      displayName: 'Run Tests'

    - task: PublishTestResults@2
      displayName: 'Publish Test Results'
      inputs:
        testResultsFiles: 'reports/test-${{ test }}*.xml'
        failTaskOnFailedTests: true
      condition: succeededOrFailed()

  - task: PublishPipelineArtifact@1 # Documentation: https://docs.microsoft.com/en-us/azure/devops/pipelines/artifacts/pipeline-artifacts?view=azure-devops&tabs=yaml-task
    displayName: 'Publish Artifacts to Shared Storage'
    condition: and(succeeded(), eq('${{ parameters.install }}', 'release'))
    inputs:
      targetPath: $(System.DefaultWorkingDirectory)/dist
      patterns: 'ms_recommenders*.whl'
      artifactName: PackageAssets

  - script: |
      conda env remove -n ${{ parameters.conda_env }} || exit -1
      rm -rf reports || exit -1
    displayName: 'Remove Conda Env'
    continueOnError: true
    condition: succeededOrFailed()
