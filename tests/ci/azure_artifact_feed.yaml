# Copyright (c) Microsoft Corporation. All rights reserved.
# Licensed under the MIT License.

# To install recommenders artifact (with manual login to Azure):
# $ pip install artifacts-keyring --pre
# Choose the version to install based on your environment:
# $ pip install reco-utils --index-url=https://pkgs.dev.azure.com/best-practices/recommenders/_packaging/recommenders/pypi/simple/
# $ pip install reco-utils-gpu --index-url=https://pkgs.dev.azure.com/best-practices/recommenders/_packaging/recommenders/pypi/simple/
# $ pip install reco-utils-pyspark --index-url=https://pkgs.dev.azure.com/best-practices/recommenders/_packaging/recommenders/pypi/simple/

# To install recommenders artifact programmatically with Personal Access Token (PAT):
# First generate the PAT:
# 1. Go to the Personal Access Tokens screen in ADO => User (icon in top right)
# 2. Generate a new token, with at least Packages/Read permissions selected
# 3. Add the PAT in the index-url=https://{feed_name}:{PAT}@pkgs.dev.azure.com/{org_name}/{project_name}/_packaging/{feed_name}/pypi/simple/
# Install dependencies and Recommenders library
# $ pip install artifacts-keyring --pre
# Choose the version to install based on your environment:
# $ pip install reco-utils --index-url=https://recommenders:{PAT}@pkgs.dev.azure.com/best-practices/recommenders/_packaging/recommenders/pypi/simple/
# $ pip install reco-utils-gpu --index-url=https://recommenders:{PAT}@pkgs.dev.azure.com/best-practices/recommenders/_packaging/recommenders/pypi/simple/
# $ pip install reco-utils-pyspark --index-url=https://recommenders:{PAT}@pkgs.dev.azure.com/best-practices/recommenders/_packaging/recommenders/pypi/simple/

# NOTE: 
# This yaml file is used to generate ADO artifacts for Recommenders library.
# We use the same yaml file to generate the library in different environments: CPU, GPU and PySpark
# When setting the pipeline, we need to define the variables env_name, env_flag and library_name
# To add the variables, go to the pipeline, press edit and then variables.
# An example of the variables we use in the GPU environment:
# env_name=artifact_reco_gpu
# env_flag=--gpu
# library_name=recommenders_gpu

# The global variables are defined in ADO/Pipelines/Library
# https://dev.azure.com/best-practices/recommenders/_library?itemType=VariableGroups
variables:
- group: LinuxAgentPool

# The pipeline will be triggered when we merge to master
pr: none
trigger:
- master

jobs:
- job: Artifact
  displayName: 'Create Recommenders artifact'
  timeoutInMinutes: 30 # how long to run the job before automatically cancelling
  pool:
    name: $(Agent_Pool)

  steps:
  - bash: |
      echo "##vso[task.prependpath]/data/anaconda/bin"
      conda env list
    displayName: 'Add Conda to PATH'

  - script: |
      conda env remove -n $(env_name) -y   
    workingDirectory: tests
    displayName: 'Conda remove'
    continueOnError: true
    condition: always() # this step will always run, even if the pipeline is canceled

  - script: |
      python ./tools/generate_conda_file.py --name $(env_name) $(env_flag)
      conda env create --quiet -f $(env_name).yaml 2> log
    displayName: 'Setup Conda Env'

  - script: |
      . /anaconda/etc/profile.d/conda.sh && \
      conda activate $(env_name) && \
      pip install wheel twine keyring artifacts-keyring && \
      pip list
    displayName: 'Install dependencies'

  - script: |
      . /anaconda/etc/profile.d/conda.sh && \
      conda activate $(env_name) && \
      rm -rf dist && \
      pip install -U pip setuptools && \
      pip install numpy==1.19 && \
      HASH=True LIBRARY_NAME=$(library_name) python setup.py sdist bdist_wheel
    displayName: 'Build wheel'

  - task: TwineAuthenticate@1
    inputs:
      artifactFeed: recommenders/recommenders
    displayName: 'Twine Authenticate'

  # Artifact package: https://dev.azure.com/best-practices/recommenders/_packaging?_a=feed&feed=recommenders
  - script: |
      . /anaconda/etc/profile.d/conda.sh && \
      conda activate $(env_name) && \
      python -m twine upload -r recommenders --config-file $(PYPIRC_PATH) dist/*.whl --verbose
    displayName: 'Upload wheel'

  - script: |
      conda env remove -n $(env_name) -y   
    workingDirectory: tests
    displayName: 'Conda remove'
    continueOnError: true
    condition: always() # this step will always run, even if the pipeline is canceled
