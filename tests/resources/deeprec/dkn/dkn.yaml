data:
  doc_size: 10 # Each feature length should be fixed at doc_size, if the number of words in document is more than doc_size, you should truncate the document to doc_size words, and if the number of words in document is less than doc_size, you should padding 0. 
  history_size: 50 # Max number of user click history, we will automatically keep the last his_size number of user click history, if users' click history is more than his_size, and we will automatically padding 0 if less than his_size.
  word_size: 12600 # word vocabulary size
  entity_size: 3987 # entity vocabulary size
  data_format: dkn
  
info:
  metrics:
  - auc
  pairwise_metrics:
  - group_auc
  - mean_mrr
  - ndcg@5;10
  show_step: 10000 # print loss every show_step batches
  
model:
  method : classification
  activation:
  - sigmoid
  attention_activation: relu
  attention_dropout: 0.0
  attention_layer_sizes: 100

  dim: 100 # word embedding dim
  use_entity: true # use entity embedding
  use_context: true # use context embedding
 
  entity_dim: 100 # entity embedding dim
  transform: true # add a transform layer for entity and context embeddings
 
  dropout:
  - 0.0
  filter_sizes: # window size of kcnn filters
  - 1
  - 2
  - 3
  layer_sizes: # layer size for final prediction score layer
  - 300
  # model_type: DKN_with_context
  model_type: dkn
  num_filters: 100 # number of filter for each filter_size in kcnn part

train:
  batch_size: 100
  embed_l1: 0.000
  embed_l2: 0.000001
  epochs: 50
  init_method: uniform
  init_value: 0.1
  layer_l1: 0.000
  layer_l2: 0.000001
  learning_rate: 0.0005
  loss: log_loss
  optimizer: adam
  save_model: False
  save_epoch : 2 # save model every save_epoch epochs
  enable_BN : True
