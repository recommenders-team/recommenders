{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<i>Copyright (c) Microsoft Corporation. All rights reserved.</i>\n",
    "\n",
    "<i>Licensed under the MIT License.</i>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.test.utils import common_texts, get_tmpfile\n",
    "from gensim.models import Word2Vec\n",
    "import time\n",
    "from utils.general import *\n",
    "import numpy as np\n",
    "import pickle\n",
    "from utils.task_helper import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MySentenceCollection:\n",
    "    def __init__(self, filename):\n",
    "        self.filename = filename\n",
    "        self.rd = None\n",
    "\n",
    "    def __iter__(self):\n",
    "        self.rd = open(self.filename, 'r', encoding='utf-8', newline='\\r\\n')\n",
    "        return self\n",
    "\n",
    "    def __next__(self):\n",
    "        line = self.rd.readline()\n",
    "        if line:\n",
    "            return list(line.strip('\\r\\n').split(' '))\n",
    "        else:\n",
    "            self.rd.close()\n",
    "            raise StopIteration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "InFile_dir = 'data_folder/my'\n",
    "OutFile_dir = 'data_folder/my/pretrained-embeddings'\n",
    "OutFile_dir_KG = 'data_folder/my/KG'\n",
    "OutFile_dir_DKN = 'data_folder/my/DKN-training-folder'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start to train word embedding... \tdone . \n",
      "time elapses: 649.8s\n"
     ]
    }
   ],
   "source": [
    "def train_word2vec(Path_sentences, OutFile_dir):     \n",
    "    OutFile_word2vec = os.path.join(OutFile_dir, r'word2vec.model')\n",
    "    OutFile_word2vec_txt = os.path.join(OutFile_dir, r'word2vec.txt')\n",
    "    create_dir(OutFile_dir)\n",
    "\n",
    "    print('start to train word embedding...', end=' ')\n",
    "    my_sentences = MySentenceCollection(Path_sentences)\n",
    "    model = Word2Vec(my_sentences, size=32, window=5, min_count=1, workers=4, iter=50)\n",
    "\n",
    "    model.save(OutFile_word2vec)\n",
    "    model.wv.save_word2vec_format(OutFile_word2vec_txt, binary=False)\n",
    "    print('\\tdone . ')\n",
    "\n",
    "Path_sentences = os.path.join(InFile_dir, 'sentence.txt')\n",
    "# train_word2vec(Path_sentences, OutFile_dir)\n",
    "\n",
    "t0 = time.time()\n",
    "train_word2vec(Path_sentences, OutFile_dir)\n",
    "t1 = time.time()\n",
    "print('time elapses: {0:.1f}s'.format(t1 - t0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/data/home/jialia/jialia/kdd2020tutorial/formal_02/recommenders/scenarios/KDD2020-tutorial\n",
      "fatal: destination path 'Fast-TransX' already exists and is not an empty directory.\n",
      "epoch 0 454690.656250\n",
      "epoch 1 376927.000000\n",
      "epoch 2 344530.656250\n",
      "epoch 3 315695.781250\n",
      "epoch 4 290692.281250\n",
      "epoch 5 268658.906250\n",
      "epoch 6 250159.546875\n",
      "epoch 7 231006.828125\n",
      "epoch 8 215869.140625\n",
      "epoch 9 200701.406250\n"
     ]
    }
   ],
   "source": [
    "## some step in transE training\n",
    "\n",
    "## https://github.com/thunlp/Fast-TransX\n",
    "\n",
    "!bash ./run_transE.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_context_embedding(entity_file, context_file, kg_file):\n",
    "    #load embedding_vec\n",
    "    entity_index = 0\n",
    "    entity_dict = {}\n",
    "    fp_entity = open(entity_file, 'r')\n",
    "    for line in fp_entity:\n",
    "        linesplit = line.strip().split('\\t')[:EMBEDDING_LENGTH]\n",
    "        linesplit = list(map(float, linesplit))\n",
    "        entity_dict[str(entity_index)] = linesplit\n",
    "        entity_index += 1\n",
    "    fp_entity.close()\n",
    "\n",
    "    #build neighbor for entity in entity_dict\n",
    "    fp_kg = open(kg_file, 'r', encoding='utf-8')\n",
    "    triple_num = fp_kg.readline()\n",
    "    triples = fp_kg.readlines()\n",
    "    kg_neighbor_dict = {}\n",
    "    for triple in triples:\n",
    "        linesplit = triple.strip().split(' ')\n",
    "        head = linesplit[0]\n",
    "        tail = linesplit[1]\n",
    "        if head not in kg_neighbor_dict:\n",
    "            kg_neighbor_dict[head] = set()\n",
    "        kg_neighbor_dict[head].add(tail)\n",
    "\n",
    "        if tail not in kg_neighbor_dict:\n",
    "            kg_neighbor_dict[tail] = set()\n",
    "        kg_neighbor_dict[tail].add(head)        \n",
    "    fp_kg.close()\n",
    "\n",
    "    context_embeddings = np.zeros([entity_index , EMBEDDING_LENGTH])\n",
    "\n",
    "    for entity in entity_dict:\n",
    "        if entity in kg_neighbor_dict:\n",
    "            context_entity = kg_neighbor_dict[entity]\n",
    "            context_vecs = []\n",
    "            for c_entity in context_entity:\n",
    "                context_vecs.append(entity_dict[c_entity])\n",
    "\n",
    "            context_vec = np.mean(np.asarray(context_vecs), axis=0)\n",
    "            context_embeddings[int(entity)] = context_vec\n",
    "\n",
    "    np.savetxt(context_file, context_embeddings, delimiter='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### build context embedding\n",
    "EMBEDDING_LENGTH = 32\n",
    "entity_file = os.path.join(OutFile_dir_KG, 'entity2vec.vec') \n",
    "context_file = os.path.join(OutFile_dir_KG, 'context2vec.vec')   \n",
    "kg_file = os.path.join(OutFile_dir_KG, 'train2id.txt')   \n",
    "gen_context_embedding(entity_file, context_file, kg_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_np_from_txt(\n",
    "        os.path.join(OutFile_dir_KG, 'entity2vec.vec'),\n",
    "        os.path.join(OutFile_dir_DKN, 'entity_embedding.npy'),\n",
    "    )\n",
    "load_np_from_txt(\n",
    "        os.path.join(OutFile_dir_KG, 'context2vec.vec'),\n",
    "        os.path.join(OutFile_dir_DKN, 'context_embedding.npy'),\n",
    "    )\n",
    "format_word_embeddings(\n",
    "    os.path.join(OutFile_dir, 'word2vec.txt'),\n",
    "    os.path.join(InFile_dir, 'word2idx.pkl'),\n",
    "    os.path.join(OutFile_dir_DKN, 'word_embedding.npy')\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "reco_gpu_kdd",
   "language": "python",
   "name": "reco_gpu_kdd"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
