# ---------------------------------------------------------
# Copyright (c) Microsoft Corporation. All rights reserved.
# Licensed under the MIT License.
# ---------------------------------------------------------

# ToDo: workflow that gets triggered on a PR to main/staging
# and submits an experiment to run tests on AML cluster
name: trigger-aml-notebooks-spark-unit-tests

on:
  pull_request:
    branches: [ staging, main ]

  # enable manual trigger
  workflow_dispatch:
    input:
      tags:
        description: 'Tags to label this manual run (optional)'
        default: 'Anything to describe this manual run'

env:
  CLUSTER_NAME: "cpu-aml-cluster"
  EXP_NAME: "notebooks_spark_unit_tests"
  RG: "pradjoshi-aml-rg"
  WS: "pradjoshi-aml-ws"
  TEST_MARKERS: '"notebooks and spark and not gpu and not experimental"'
  TEST_LOGS_PATH: '"./test_logs.log"'
  PYTEST_EXIT_CODE: "pytest_exit_code.log"

jobs:
  notebooks-spark-unit-tests:
    runs-on: ubuntu-latest
    steps:
      - name: Check out repository code
        uses: actions/checkout@v2
      - name: Setup python
        uses: actions/setup-python@v2
        with: 
          python-version: "3.8"
      - name: Install azureml-core and azure-cli on a GitHub hosted server
        run: pip install azureml-core azure-cli
      - name: Log in to Azure
        uses: azure/login@v1
        with:
          creds: ${{secrets.AML_TEST_CREDENTIALS}}
      - name: Install wheel package
        run: pip install wheel
      - name: Create wheel from setup.py
        run: python setup.py bdist_wheel
      - name: Submit a test to AML
        run: >-
            python tests/ci/aml_tests_github_actions/submit_azureml_pytest.py --clustername ${{env.CLUSTER_NAME}}
            --subid ${{secrets.AML_TEST_SUBID}} --reponame "recommenders" --branch "pradjoshi/aml_tests"
            --rg ${{env.RG}} --wsname ${{env.WS}} --testmarkers ${{env.TEST_MARKERS}} --expname ${{env.EXP_NAME}}
            --testlogs ${{env.TEST_LOGS_PATH}} --add_spark_dependencies
      - name: Print test logs
        run: cat ${{env.TEST_LOGS_PATH}}
      - name: Get exit status
        id: exit_status
        run: echo ::set-output name=code::$(cat ${{env.PYTEST_EXIT_CODE}})
      - name: Check Success/Failure
        if: ${{ steps.exit_status.outputs.code != 0 }}
        uses: actions/github-script@v3
        with:
          script: |
              core.setFailed('All tests did not pass!')
