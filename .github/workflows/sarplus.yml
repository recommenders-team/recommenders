# This workflow will run tests and do packaging for contrib/sarplus.
#
# Refenreces:
#   * [GitHub Actions doc](https://docs.github.com/en/actions)
#   * GitHub Actions workflow templates
#       + [python package](https://github.com/actions/starter-workflows/blob/main/ci/python-package.yml)
#       + [python publish](https://github.com/actions/starter-workflows/blob/main/ci/python-publish.yml)
#       + [scala](https://github.com/actions/starter-workflows/blob/main/ci/scala.yml)
#   * [GitHub hosted runner - Ubuntu 20.04 LTS](https://github.com/actions/virtual-environments/blob/main/images/linux/Ubuntu2004-README.md)
#   * [Azure Databirkcs runtime releases](https://docs.microsoft.com/en-us/azure/databricks/release-notes/runtime/releases)
#   * Package publish
#       + [Publishing package distribution releases using GitHub Actions CI/CD workflows](https://packaging.python.org/guides/publishing-package-distribution-releases-using-github-actions-ci-cd-workflows/)
#       + [pypa/gh-action-pypy-publish](https://github.com/pypa/gh-action-pypi-publish)


name: sarplus package

on:
  push:
    paths:
      - contrib/sarplus/python/**
      - contrib/sarplus/scala/**
      - .github/workflows/sarplus.yml

env:
  PYTHON_ROOT: ${{ github.workspace }}/contrib/sarplus/python
  SCALA_ROOT: ${{ github.workspace }}/contrib/sarplus/scala

jobs:
  python:
    runs-on: ubuntu-latest
    strategy:
      matrix:
        python-version: ["3.6", "3.7", "3.8", "3.9", "3.10"]
    steps:
      - uses: actions/checkout@v2

      - name: Set up Python ${{ matrix.python-version }}
        uses: actions/setup-python@v2
        with:
          python-version: ${{ matrix.python-version }}

      - name: Install dependencies
        run: |
          python -m pip install -U build pip twine
          python -m pip install -U flake8 pytest pytest-cov scikit-learn

      - name: Lint with flake8
        run: |
          cd "${PYTHON_ROOT}"
          # See https://flake8.pycqa.org/en/latest/user/index.html
          flake8 .

      - name: Package and check
        run: |
          cd "${PYTHON_ROOT}"
          cp ../VERSION ./
          python -m build --sdist
          python -m twine check dist/*

      - name: Testing
        env:
          ACCESS_TOKEN: ${{ secrets.SARPLUS_TESTDATA_ACCESS_TOKEN }}
        run: |
          cd "${PYTHON_ROOT}"
          python -m pip install dist/*.gz

          cd "${SCALA_ROOT}"
          export SPARK_VERSION=$(python -m pip show pyspark | grep -i version | cut -d ' ' -f 2)
          SPARK_JAR_DIR=$(python -m pip show pyspark | grep -i location | cut -d ' ' -f2)/pyspark/jars
          SCALA_JAR=$(ls ${SPARK_JAR_DIR}/scala-library*)
          HADOOP_JAR=$(ls ${SPARK_JAR_DIR}/hadoop-client-api*)
          SCALA_VERSION=${SCALA_JAR##*-}
          export SCALA_VERSION=${SCALA_VERSION%.*}
          HADOOP_VERSION=${HADOOP_JAR##*-}
          export HADOOP_VERSION=${HADOOP_VERSION%.*}
          sbt ++"${SCALA_VERSION}"! package

          cd "${PYTHON_ROOT}"
          pytest --token "${ACCESS_TOKEN}" ./tests
          echo "sarplus_version=$(cat ../VERSION)" >> $GITHUB_ENV

      - name: Upload Python package
        if: github.ref == 'refs/heads/main' && matrix.python-version == '3.10'
        uses: actions/upload-artifact@v2
        with:
          name: pysarplus-${{ env.sarplus_version }}
          path: ${{ env.PYTHON_ROOT }}/dist/*.gz

      - name: Publish package to PyPI
        if: github.ref == 'refs/heads/main' && matrix.python-version == '3.10'
        uses: pypa/gh-action-pypi-publish@release/v1
        with:
          user: __token__
          password: ${{ secrets.SARPLUS_TEST_PYPI_API_TOKEN }}
          repository_url: https://test.pypi.org/legacy/
          packages_dir: ${{ env.PYTHON_ROOT }}/dist/
          skip_existing: true

  scala-test:
    runs-on: ubuntu-latest
    strategy:
      matrix:
        include:
          - scala-version: "2.12.10"
            spark-version: "3.0.1"
            hadoop-version: "2.7.4"
            databricks-runtime: "ADB 7.3 LTS"

          - scala-version: "2.12.10"
            spark-version: "3.1.2"
            hadoop-version: "2.7.4"
            databricks-runtime: "ADB 9.1 LTS"

          - scala-version: "2.12.14"
            spark-version: "3.2.0"
            hadoop-version: "3.3.1"
            databricks-runtime: "ADB 10.0"

    steps:
      - uses: actions/checkout@v2

      - name: Test
        run: |
          cd "${SCALA_ROOT}"
          export SPARK_VERSION="${{ matrix.spark-version }}"
          export HADOOP_VERSION="${{ matrix.hadoop-version }}"
          sbt ++${{ matrix.scala-version }}! test

  scala-package:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v2

      - name: Package
        env:
          GPG_KEY: ${{ secrets.SARPLUS_GPG_PRI_KEY_ASC }}
        run: |
          # generate artifacts
          cd "${SCALA_ROOT}"
          export SPARK_VERSION="3.1.2"
          export HADOOP_VERSION="2.7.4"
          export SCALA_VERSION=2.12.10
          sbt ++${SCALA_VERSION}! package
          sbt ++${SCALA_VERSION}! packageDoc
          sbt ++${SCALA_VERSION}! packageSrc
          sbt ++${SCALA_VERSION}! makePom
          export SPARK_VERSION="3.2.0"
          export HADOOP_VERSION="3.3.1"
          export SCALA_VERSION=2.12.14
          sbt ++${SCALA_VERSION}! package

          # sign with GPG
          cd target/scala-2.12
          gpg --import <(cat <<< "${GPG_KEY}")
          for file in {*.jar,*.pom}; do gpg -ab "${file}"; done

          # bundle
          jar cvf sarplus-bundle_2.12-$(cat ../VERSION).jar *.jar *.pom *.asc
          echo "sarplus_version=$(cat ../VERSION)" >> $GITHUB_ENV
          
      - name: Upload Scala bundle
        uses: actions/upload-artifact@v2
        with:
          name: sarplus-bundle_2.12-${{ env.sarplus_version }}
          path: ${{ env.SCALA_ROOT }}/target/scala-2.12/sarplus-bundle_2.12-${{ env.sarplus_version }}.jar
