# This workflow will run tests and do packaging for contrib/sarplus.
#
# Refenreces:
#   * [GitHub Actions doc](https://docs.github.com/en/actions)
#   * GitHub Actions workflow templates
#       + [python package](https://github.com/actions/starter-workflows/blob/main/ci/python-package.yml)
#       + [python publish](https://github.com/actions/starter-workflows/blob/main/ci/python-publish.yml)
#       + [scala](https://github.com/actions/starter-workflows/blob/main/ci/scala.yml)
#   * [GitHub hosted runner - Ubuntu 20.04 LTS](https://github.com/actions/virtual-environments/blob/main/images/linux/Ubuntu2004-README.md)
#   * [Azure Databirkcs runtime releases](https://docs.microsoft.com/en-us/azure/databricks/release-notes/runtime/releases)
#   * Package publish
#       + [Publishing package distribution releases using GitHub Actions CI/CD workflows](https://packaging.python.org/guides/publishing-package-distribution-releases-using-github-actions-ci-cd-workflows/)
#       + [pypa/gh-action-pypy-publish](https://github.com/pypa/gh-action-pypi-publish)


name: sarplus package

on:
  push:
    paths:
      - contrib/sarplus/python/**
      - contrib/sarplus/scala/**
      - .github/workflows/sarplus.yml

env:
  PYTHON_ROOT: ${{ github.workspace }}/contrib/sarplus/python
  SCALA_ROOT: ${{ github.workspace }}/contrib/sarplus/scala
  SARPLUS_VERSION: 0.5.0

jobs:
  python:
    runs-on: ubuntu-latest
    strategy:
      matrix:
        python-version: ["3.6", "3.7", "3.8", "3.9", "3.10"]
    steps:
      - uses: actions/checkout@v2

      - name: Set up Python ${{ matrix.python-version }}
        uses: actions/setup-python@v2
        with:
          python-version: ${{ matrix.python-version }}

      - name: Install dependencies
        run: |
          python -m pip install -U build pip twine
          python -m pip install -U flake8 pytest pytest-cov scikit-learn

      - name: Lint with flake8
        run: |
          cd "${PYTHON_ROOT}"
          # See https://flake8.pycqa.org/en/latest/user/index.html
          flake8 .

      - name: Package and check
        run: |
          cd "${PYTHON_ROOT}"
          sed -i -E "s/version[[:space:]]*=[[:space:]]*.*,/version=\"${SARPLUS_VERSION}\",/" setup.py
          python -m build --sdist
          python -m twine check dist/*

      - name: Testing
        env:
          ACCESS_TOKEN: ${{ secrets.SARPLUS_TESTDATA_ACCESS_TOKEN }}
        run: |
          cd "${PYTHON_ROOT}"
          python -m pip install dist/*.gz

          cd "${SCALA_ROOT}"
          export SPARK_VERSION=$(python -m pip show pyspark | grep -i version | cut -d ' ' -f 2)
          SPARK_JAR_DIR=$(python -m pip show pyspark | grep -i location | cut -d ' ' -f2)/pyspark/jars
          SCALA_JAR=$(ls ${SPARK_JAR_DIR}/scala-library*)
          HADOOP_JAR=$(ls ${SPARK_JAR_DIR}/hadoop-client-api*)
          SCALA_VERSION=${SCALA_JAR##*-}
          export SCALA_VERSION=${SCALA_VERSION%.*}
          HADOOP_VERSION=${HADOOP_JAR##*-}
          export HADOOP_VERSION=${HADOOP_VERSION%.*}
          export VERSION="${SARPLUS_VERSION}"
          sbt ++"${SCALA_VERSION}"! package

          cd "${PYTHON_ROOT}"
          pytest --token "${ACCESS_TOKEN}" ./tests

      - name: Publish package to PyPI
        if: github.ref == 'refs/heads/main' && matrix.python-version == '3.10'
        uses: pypa/gh-action-pypi-publish@release/v1
        with:
          user: __token__
          password: ${{ secrets.SARPLUS_TEST_PYPI_API_TOKEN }}
          repository_url: https://test.pypi.org/legacy/
          packages_dir: ${{ env.PYTHON_ROOT }}/dist/
          skip_existing: true

  scala:
    runs-on: ubuntu-latest
    strategy:
      matrix:
        include:
          - scala-version: "2.12.10"
            spark-version: "3.0.1"
            hadoop-version: "2.7.4"
            databricks-runtime: "ADB 7.3 LTS"

          - scala-version: "2.12.10"
            spark-version: "3.1.2"
            hadoop-version: "2.7.4"
            databricks-runtime: "ADB 9.1 LTS"

          - scala-version: "2.12.14"
            spark-version: "3.2.0"
            hadoop-version: "3.3.1"
            databricks-runtime: "ADB 10.0"

    steps:
      - uses: actions/checkout@v2

      - name: Testing and Packaging
        run: |
          cd "${SCALA_ROOT}"
          export VERSION="${SARPLUS_VERSION}"
          export SPARK_VERSION="${{ matrix.spark-version }}"
          export HADOOP_VERSION="${{ matrix.hadoop-version }}"
          sbt ++${{ matrix.scala-version }}! test
          sbt ++${{ matrix.scala-version }}! package
          SCALA_VERSION=${{ matrix.scala-version }}
          echo "scala_binary_version=${SCALA_VERSION%.*}" >> $GITHUB_ENV
      - name: Upload Scala package
        uses: actions/upload-artifact@v2
        with:
          name: scala_${{ matrix.scala-version }}_s${{ matrix.spark-version }}_h${{ matrix.hadoop-version }}-${{ env.SARPLUS_VERSION }} (${{ matrix.databricks-runtime }})
          path: ${{ env.SCALA_ROOT }}/target/scala-${{ env.scala_binary_version }}/*.jar
#       - name: Publish Scala package
#         if: github.ref == 'refs/heads/main'
#         run: 
